---
title: 模式识别 第二章 统计决策方法 贝叶斯决策理论
date: 2017-03-26 18:47:48
categories: ML/卢晓春 模式识别引论
mathjax: true
tags: [Machine Learning]
---
<!--more-->
# 卢晓春 模式识别引论
## 第二章 统计决策方法 贝叶斯决策理论

> 先验概率=已知的类别的概率，如二分类$P(w_1)+P(w_2)=1$
> 类条件概率密度函数：$P(x|w_1),P(x|w_2)...P(x|w_n)$
> 后验概率：分类判决的依据$P(w_1|x)+P(w_2|x)=1$
> 比如某地区的癌细胞概率，病人

不同阈值标准：

* 最小错误率贝叶斯决策
$$P(w_i|x)=\frac{P(x|w_i)*P(w_i)}{P(x)}=\frac{P(x|w_i)P(w_i)}{\sum_{j=i}^{2}P(x|w_j)P(w_j)},i=1,2$$
即求后验概率更大的情况i=$argumax(P(w_i|x))$，又$P(w_i|x)$中分母相同，
即求i=$argumax(P(x|w_i)*P(w_i))$
整理成似然比形式：
$$\begin{split} P(x|w_1)*P(w_1)&>P(x|w_2)*P(w_2) \\
l(x)=\frac{P(x|w_1)}{P(x|w_2)}&>\frac{P(w_2)}{P(w_1)}
\begin{cases}
true, &\quad x \in w_1 \\
false,&\quad x \in w_2
\end{cases}
\end{split} $$
例题：
>![Paste_Image.png](http://upload-images.jianshu.io/upload_images/2812342-5428d3f301e42a84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
某地区正常细胞和异常细胞的先验概率为$P(w_1)=0.9,P(w_2)=0.1$
某人的细胞观察值x，根据类条件概率密度函数,$P(x|w_1)=0.2,P(x|w_2)=0.4$
因为$\frac{P(x|w_1)}{P(x|w_2)}>\frac{P(w_2)}{P(w_1)}$，所以属于第一类，判定是正常细胞。
*注：图中交叉部分，实际为绿可是判定为红，或则反之，都是错误的判定*
* 最小风险贝叶斯决策
决策表

|决策|类别|类别|
|-:|:-:|:-:|
|风险值|$\omega_1$|$\omega_2$|
|$$a_1$$|$$\lambda_{11}$$|$$\lambda_{12}$$|
|$$a_2$$|$$\lambda_{21}$$|$$\lambda_{22}$$|

将后验概率$P(w_i|x)$乘上风险
$$R(a_1|x)=\sum_{j=1}^2\lambda_{1j}P(w_j|x) $$
$$R(a_2|x)=\sum_{j=1}^2\lambda_{2j}P(w_j|x) $$
比较，R较小的那个为最终决策。

例题
> |决策|类别|类别|
|:-:|:-:|:-:|
|风险值|$\omega_1$|$\omega_2$|
|$a_1$|0|6|
|$a_2$|1|0|
> 某地区正常细胞和异常细胞的先验概率为
$$P(w_1)=0.9,P(w_2)=0.1$$
某人的细胞观察值x，根据类条件概率密度函数,
$$P(x|w_1)=0.2,P(x|w_2)=0.4$$
因为
$$R(a_1|x)=\sum_{j=1}^2 \lambda_{1j}P(w_j|x)=0+\lambda_{12}P(w_2|x)=1.092$$，
$$R(a_2|x)=\sum_{j=1}^2 \lambda_{1j}P(w_j|x)=\lambda_{21}P(w_1|x)+0=0.818$$
所以为了风险最小，判定为第二类

* Neyman-Person决策 奈曼皮尔逊：限定一类错误率为常数而使另一类错误率最小的决策规则。
    * 第一类错误率$$P_1(e)=\int_{R_2}P(x|w_1)dx$$
    * 第二类错误率$$P_2(e)=\int_{R_1}P(x|w_2)dx$$
    * 整个特征空间$R=R_1+R_2$
【Lagrange乘子法】：
$$mini \gamma = P_1(e)+\lambda(P_2(e)-\epsilon_0)$$
又错误=1-正确：
$$P_1(e)=\int_{R_2}P(x|w_1)dx=1-\int_{R_1}P(x|w_1)dx$$
代入：
$$\begin{split}
\gamma &= 1-\int_{R_1}P(x|w_1)dx+\lambda P_2(e)-\lambda \epsilon_0 \\
&=(1-\lambda \epsilon_0)+\lambda \int_{R_1}P(x|w_2)dx-\int_{R_1}P(x|w_1)dx \\
&=(1-\lambda \epsilon_0)+ \int_{R_1}[\lambda P(x|w_2)-P(x|w_1)]dx \\
\end{split}$$ 
分别对$\lambda$和分界面t求导，得到
$$\lambda=\frac{P(x|w_1)}{P(x|w_2)}$$
且
$$\int_{R_1}P(x|w_2)dx=\epsilon_0$$
而在分界面处，可以看到$$[\lambda P(x|w_2)-P(x|w_1)]<0$$才能使得值尽量小
所以
$$
l(x)=\frac{P(x|w_1)}{P(x|w_2)}>\lambda ,
\begin{cases}
true, &\quad x \in w_1 \\
false,&\quad x \in w_2
\end{cases}$$
* 最小最大决策：保守的，在最差情况下有好的结果。
* 另有ROC曲线见吴恩达《机器学习》课程

