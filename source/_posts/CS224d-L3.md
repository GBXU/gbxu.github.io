
---
title: CS224d L3 高级的词向量表示
date: 2017-03-27 19:43:56
categories: NLP/CS224d
tags: [CS224d,NLP]
---
<!--more-->
Advanced word vector representations: language models, softmax, single layer networks

>$$J(\theta)=\frac{1}{T}\sum_{t=1}^{T}\sum_{-c<=j<=c,j\neq0}logP(w_{t+j}|w_t)$$
$$P(w_0|w_t)=\frac{e^{v_{w_0}^{'T}v_{w_i}}}{\sum_{w=1}^{W}e^{v_{w}^{'T}v_{w_i}}}$$

## BGD/SGD/MBGD
### BGD 批量梯度下降(Batch Gradient Descent)
更新每一参数时都使用所有的样本来进行更新

* 优点：全局最优解；
* 缺点：当样本数目很多时，训练过程会很慢。 
>
$\theta^{new}=\theta^{old}-\alpha\frac{\partial}{\partial\theta^{old}}J(\theta)$
![image.png](//upload-images.jianshu.io/upload_images/2812342-92c74b252d29695b.png)


### SGD 随机梯度下降(Stochastic Gradient Descent)
SGD是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了.

对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。

但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。

* 优点：训练速度快；
* 缺点：准确度下降，并不是全局最优。 

>$\theta^{new}=\theta^{old}-\alpha\frac{\partial}{\partial\theta^{old}}J_t(\theta)$
![image.png](//upload-images.jianshu.io/upload_images/2812342-e46c6560083ae7ba.png)

### 小批量梯度下降法MBGD(Mini-batch Gradient Descent)
BGD和SGD的折衷

## CBOW/Skip-gram
![image.png](http://upload-images.jianshu.io/upload_images/2812342-d879b0d8a7090274.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### Continuous Bag-of-Words Model （CBOW）
![image.png](http://upload-images.jianshu.io/upload_images/2812342-203531e358da4e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
1. 输入单词矩阵V，有one-hot的X标明。这样$v_i=VX_i$
2. $v_{avg}=\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m}$求出输入层的平均值
3. Generate a score vector $z=Uv_{avg}$，和output和input average相乘
4. 然后用softmax方法，$y_{'}=softmax(z)$，得到的$y_{'}$也是one hot的向量

* Loss function：[cross entropy交叉熵](http://blog.csdn.net/rtygbwwwerr/article/details/50778098),交叉熵简单地说，就是一个事件发生的概率越大，则它所携带的信息量就越小。
所以有$H(y_{'},y)=-\sum _{j=1}^{|V|}y_jlog(y_j^{'})$，又因为y为one hot，所以$H(y_{'},y)$化简为$H(y_{'},y)=-y_jlog(y_j^{'})$。

    * 即得出的结果符合上下文的$H ( ŷ, y ) =
    − 1 log ( 1 ) = 0$，
    * 而得出的结果不符合上下文的$H ( ŷ, y ) = − 1 log ( 0.01 ) ≈ 4.605$
* 而对于cost function：由softmax可得
如果是对于每一个测试样本，都有
$$minimize J=-logP(w_{c}|w_{c-m}..w_{c-1}w_{c+1}..w_{c+m})\\
=-logP(u_c|v^{'})\\
=-log\frac{e^{u_{c}^{T}v^{'}}}{\sum_{j=1}^{|V|}e^{u_{j}^{T}v^{'}}}\\
=-u_{c}^{T}v^{'}+log\sum_{j=1}^{|V|}e^{u_{j}^{T}v^{'}}$$
* 之后用梯度去更新相关词向量$u_c$和$v_j$

### Continuous Skip-gram Model (Skip-gram)
该模型是用center word预测the surrounding words。
![image.png](http://upload-images.jianshu.io/upload_images/2812342-eeb3cdfff6b030b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
1. 输入中心词x为one hot向量
2. 和CBOW一样，单词$v_c=VX$
3. 因为只有一个，求平均是$v_{avg}=v_c$
4. 利用$u=Uv_c$生成2m个单词$u_{c-m}，u_{c-m+1}..u_{c+m}$的score
5. Turn each of the scores into probabilities, y = softmax (u)
6. $y_{c-m}，y_{c-m+1}..y_{c+m}$为真实y，而$y^{'}_{c-m}，y^{'}_{c-m+1}..y^{'}_{c+m}$为求出来的
![image.png](http://upload-images.jianshu.io/upload_images/2812342-b643fe547dc56f7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


### Negative Sampling


## Appendix
* [第三讲：高级的词向量PPT中文版](http://www.52nlp.cn/%E6%96%AF%E5%9D%A6%E7%A6%8F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E9%AB%98%E7%BA%A7%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA)
* CBOW/Skip-gram/nagative sampling见[CS224d-L1/L2/L3前部分note](http://cs224d.stanford.edu/lecture_notes/notes1.pdf)
* [深度学习与自然语言处理(2)_斯坦福cs224d Lecture 2笔记中文版 ](http://blog.csdn.net/han_xiaoyang/article/details/51648483)
* softmax算法见[UFLDL教程的Softmax回归](http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)
* 关于The skip-gram model and negative sampling，详细可见论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

