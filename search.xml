<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Personal Goals]]></title>
    <url>%2F2025%2F11%2F07%2FGoal%2F</url>
    <content type="text"><![CDATA[小的时候，做什么事能让时间过得飞快并让你快乐，这个答案就是你在尘世的追求。 在2021年，中国共产党成立一百周年，全面建成小康社会； 在2049年，新中国成立一百周年，建成富强民主文明和谐的社会主义现代化国家。 “在隆冬，我终于知道，我身上有一个不可战胜的夏天。” –阿尔贝·加缪 one piece of advice to give to students-Andrew Ng’s Answer: When deciding how to spend your time, I recommend you take into account two criteria: Whether what you’re doing can change the world. How much you’ll learn.]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>Goals</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转：你想戴一顶什么样的博士帽 李开复]]></title>
    <url>%2F2017%2F07%2F31%2F%E8%BD%AC%EF%BC%9A%E4%BD%A0%E6%83%B3%E6%88%B4%E4%B8%80%E9%A1%B6%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E5%8D%9A%E5%A3%AB%E5%B8%BD%2F</url>
    <content type="text"><![CDATA[在我进入卡内基梅隆大学攻读计算机博士学位时，系主任曾对我讲，当你拿到你的博士学位时，你应该成为你所从事的研究领域里世界第一的专家。这句话对于初出茅庐的我来说简直高不可攀，但也让我 踌躇满志、跃跃欲试。就这样，在经过五年寒窗、夜以继日的努力工作后，他所期待的结果就那么自然而然地出现了。一个打算攻读博士学位的人，就应该给自己树 立一个很高的目标。如果没有雄心壮志，就千万不要自欺欺人，也许经商或从事其它工作，会有更大的成绩。 在目标确立之后，我建议你为自己设计一个三年的学习和科研计划。首先，你需要彻底地了解在相关领域他人已有的工作和成绩。然后再提出自己的想法和见解，做脚踏实地的工作。另外，还要不断跟踪这个领域的最新研究进展。只有这样，才可以把握好方向，避免重复性工作，把精力集中在最有价值的研究方向上。 在学术界，人们普遍认为“名师出高徒”。可见导师在你的成长道路中作用是多么的大。所以，你应该主动去寻找自己所研究的领域里最好的老师。除了你的老师之外，你还应该去求教于周围所有的专家。更不要忘了常去求教“最博学的老师”- Internet！现在，几乎所有的论文、研究结果、先进想法都可以在网上找到。我还鼓励你直接发电子邮件去咨询一些世界公认的专家和教授。以我的经验，对于这样的邮件，他们中的大部分都会很快给你回复。 我在攻读博士学位时，每周工作七天，每天工作16个小时，大量的统计结果和分析报告几乎让我崩溃。那时，同领域其他研究人员采用的是与我不同的传统方法。我的老师虽然支持我，但并不认可我的研究方向。我也曾不止一次地怀疑自己的所作所为是否真的能够成功。但终于有一天，在半夜三点时做出的一个结果让我感受到了成功的滋味。后来，研究有了突飞猛进的进展，导师也开始采用我的研究方法。我的博士论文使我的研究成为自然语言研究方面当时最有影响力的工作之一。读博士不是一件轻松的事，切忌浮躁的情绪，而要一步一个脚印，扎扎实实地工作。也不可受一些稍纵即逝的名利的诱惑，而要200％的投入。也许你会疲劳，会懊悔，会迷失方向，但是要记住，你所期待的成功和突破也正孕育其中。那种一切都很顺利，谁都可以得到的工作和结果，我相信研究价值一定不高。 从一定意义上讲，一个人如果打算一辈子从事研究工作，那么从他在读博士学位期间所形成的做事习惯、研究方法和思维方式基本上就可以判断出他未来工作的轮廓。所以，你一定要做一个“有心人”，充分利用在校的时间，为自己的将来打好基础。 写在最后的话 上述一些观点，是我在与同学们交往过程中的一些感受。我希望这些建议和想法能对正在未来之路上跋涉的你们有所启发，能对你们目前的学习有所帮助。或许因为观 点不同、人各有志，或许因为忠言逆耳，这封信可能无法为每一位同学所接受。但是只要一百位阅读这封信的同学中有一位从中受益，这封信就已经比我所作的任何研究都更有价值。我真诚地希望，在新的世纪，中国学生无论是在国内，还是国外；无论是做研究，还是经商，都显得更成熟一些，成功的机率更大一些。]]></content>
      <categories>
        <category>分享</category>
      </categories>
      <tags>
        <tag>分享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘-L11-L12 观念抽取及深度学习的介绍]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L11-L12%2F</url>
    <content type="text"><![CDATA[L11 Opinion mining简单聊了聊一篇文章如何抽取出abstract比如：每句话一个向量，聚类成3个类，找出中心的句子来代表这个类的主题句，然后组成abstract L12 深度学习请赵俊杰学长，介绍CNN、RNN及他的毕设。其中CNN可以参照该博客Demo： 上图是一个很有趣的可视化模型，链接：两层CNN手写体识别 后记机器学习更重要的是项目经验，一个具体的问题用什么类型的算法，每个算法的调参、寻优、特征的选择、算法内的小模块可以再组合]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘-L10]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L10%2F</url>
    <content type="text"><![CDATA[L10 Association Rules关联规则 (Groups and Topics for Course Project confirmed)参考博客参考博客2 Association Rule Mining 基础定义： Itemset A collection of one or more items Example: {Milk, Bread, Diaper} k-itemset An itemset that contains k items Support count ($\delta$) Frequency of occurrence of an itemset E.g. $\delta$ ({Milk, Bread,Diaper}) = 2 Support Fraction of transactions that contain an itemset E.g. s({Milk, Bread, Diaper}) = 2/5 Frequent Itemset An itemset whose support is greater than or equal to a minsup threshold Association Rule An implication expression of the formX -&gt; Y, where X and Y are itemsets Example: {Milk, Diaper} -&gt; {Beer} Rule Evaluation Metrics Support (s) 支持度 Fraction of transactions that contain both X and Y Confidence (c) 置信度 Measures how often items in Yappear in transactions that contain X Maximal Frequent Itemset：An itemset is maximal frequent if none of its immediate supersets is frequent 子集都不是高频的 Closed Itemset：An itemset is closed if none of its immediate supersets has the same support as the itemset 子集的support都比自己小 小结论：Frequent Itemsets &gt; Closed Frequent Itemsets&gt; Maximal Frequent Itemsets Association Rule Mining Task大多数关联规则挖掘算法通常采用的一种策略是，将关联规则挖掘任务分解为如下两个主要的子任务。 频繁项集产生：其目标是发现满足最小支持度阈值的所有项集，这些项集称作频繁项集（frequent itemset）。 规则的产生：其目标是从上一步发现的频繁项集中提取所有高置信度的规则，这些规则称作强规则（strong rule）。 通常，频繁项集产生所需的计算开销远大于产生规则所需的计算开销。 最容易想到、也最直接的进行关联关系挖掘的方法或许就是暴力搜索（Brute-force）的方法： Brute-force 蛮力法 List all possible association rules Compute the support and confidence for each rule Prune rules that fail the minsup and minconf thresholds 剪枝 然而，由于Brute-force的计算量过大，所以采样这种方法并不现实！ 格结构（Lattice structure）常被用来枚举所有可能的项集。如下图所示为I={a,b,c,d,e}的项集格。一般来说，排除空集后，一个包含k个项的数据集可能产生$2^k−1$个频繁项集。由于在实际应用中k的值可能非常大，需要探查的项集搜索空集可能是指数规模的。 图上所有的候选项集，如果被transactions包含，则数量增加。这样需要计算$O(NMw)$,其中$M=2^d$ Apriori算法Apriori定律1：如果一个集合是频繁项集，则它的所有子集都是频繁项集。 Apriori定律2：如果一个集合不是频繁项集，则它的所有超集都不是频繁项集。 举例：假设集合{A}不是频繁项集，即A出现的次数小于 min_support，则它的任何超集如{A,B}出现的次数必定小于min_support，因此其超集必定也不是频繁项集。 下图表示当我们发现{A,B}是非频繁集时，就代表所有包含它的超级也是非频繁的，即可以将它们都剪除。 效果： 6+15+20=41 Apriori Algorithm 算法描述 Let k=1 Generate frequent itemsets of length 1 Repeat until no new frequent itemsets areidentified Generate length (k+1) candidate itemsets from length kfrequent itemsets Prune candidate itemsets containing subsets of length kthat are infrequent Count the support of each candidate by scanning the DB Eliminate candidates that are infrequent, leaving onlythose that are frequent加快统计 加快support的计算：candidate itemsets加入到一个hash tree中 加快condition的计算： subset operation 到 subset operation using hash tree FP-Tree算法示例： 如何通过FP树提取规则规则的提取由最后的结点e开始，按照与建树相反的顺序进行提取。（假定支持度计数为2）以结点e为例： 1、收集包含e结点的所有路径，这些初始的路径称为前缀路径（prefixpath）。 2、如图所示的前缀路径，通过把与结点e相关联的支持度计数相加得到e的支持度计数。假定最小的支持度为2，因为{e}的支持度是3所以它是频繁项集。 3、由于{e}是频繁的，因此算法必须解决发现以de、ce、be和ae结尾的频繁项集的子问题。在解决这些子问题之前，必须先将前缀路径转换成条件fp树。除了用于发现以某特定后缀结尾的频繁项集之外，条件fp树的结果与fp树类似。 条件fp树通过以下步骤得到： a、首先，必须更新前缀路径上的支持度计数。因为某些计数包含哪些不含项e的事务。例如图中最右边路径null-&gt;b:2-&gt;c:2-&gt;e:1,包含并不包含项e事务{b、c}。因此，必须将该前缀路径上的计数调整为1，以反映包含{b，c，e}的事务的实际个数。 b、删除e的结点，修剪前缀路径.删除这些结点是因为，沿这些前缀路径的支持度计数已经更新，以反映包含e的那些事务，并且以发现de、ce、be和ae结尾的频繁项集问题的子问题不再需要结点e信息。 c、更新沿前缀路径上的支持度计数后，某些项可能不是频繁的，删除那些不是频繁的项。 4、FP增长使用e的条件fp树来解决发现以de、ce、be和ae结尾的频繁项子问题。为了发现以de结尾的频繁项集，从项e的条件fp树来收集d的所有前缀路径。通过将与结点d相关联的频度计数求和，得到项集{d、e}的支持度计数。因为{d、e}支持度为2，是频繁的，保留。再构建{de}的条件FP树，该树只包含一个支持度等于最小支持度的项a，所以提取出{a、d、e}。用同样的方法去判断{c、e}、{b、e}、{a、e}。提取结果如下： 后缀 频繁项集e {e},{d,e},{a,d,e},{c,e},{a,e}d {d},{c,d},{b,c,d},{b,d},{a,b,d},{a,d}c {c},{b,c},{a,b,c},{a,c}b {b},{a,b}a {a} FP算法特点：FP树是一种输入数据的压缩表示，它通过逐个读入事务，并把每个事务映射到FP树种的一条路径来构造。由于不同的事务可能会有若干个相同的项，因此它们的路径可能部分重叠。路径相互重叠的越多，使用FP树结构获得的压缩效果越好。如果FP树足够小，则能够存放在内存中，就可以直接从这个内存中的结构提出频繁项集，而不必重复扫描存放在硬盘上的数据。 评估 Association rule algorithms tend to produce too many rules many of them are uninteresting or redundant Redundant if {A,B,C} -&gt; {D} and {A,B} -&gt; {D} have same support &amp; confidence Interestingness measures can be used to prune/rank the derived patterns In the original formulation of association rules, support &amp;confidence are the only measures used Interestingness Measure 兴趣度测量指标 Statistical-based Measures 目前有的其他评价： Weka实验：超市交易]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘-L9 SVM 决策树]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L9%2F</url>
    <content type="text"><![CDATA[L9 Decision Trees &amp; SVMDecision Trees：ID3算法-CSDN博客决策树的构造思路 Choose the best attribute(s) to split the remaining instancesand make that attribute a decision node Repeat this process for recursively for each child Stop when: All the instances have the same target attribute value There are no more attributes There are no more instances ID3算法： 每次选择信息熵增益最大的属性来做决策 直到结束状态 所以需要引入两个概念： 信息熵：对于属性T，当前信息熵：S为需要决策的属性，如yes no$$Entropy(S)=\sum -P_s \log_2 Ps$$$$=-P{(s=yes)} \log2 P{(s=yes)}-P_{(s=no)} \log2 P{(s=no)}$$假设按照T属性分类后，这里假设T属性有3个值：$$Entropy(S_{(T=1)})=\sum -P_s \log_2 Ps$$$$Entropy(S{(T=2)})=\sum -P_s \log_2 Ps$$$$Entropy(S{(T=3)})=\sum -P_s \log_2 Ps$$则划分后的信息熵为：$$Entropy(S|T)=P{S_1}Entropy(S1)+P{S_2}Entropy(S2)$$$$+P{S_3}*Entropy(S_3)$$ 信息熵增益$$IG(T)=Entropy(S)-Entropy(S|T)$$ ID3的缺点 Uses expected entropy reduction, not actual reduction 数据需是离散值决策树的缺点 决策快但是构造时候慢 errors propagating 一个结点时出错下面的判断都是错的 SVMgeneral idea在样本空间里，$w^Tx+b=0$划分平面，且要使得 $\frac{2}{||w||}$ 最大,等价于$||w||^2=w^Tw$最小对于测试数据，\begin{cases}w^Tx_i+b \geq 1, &amp;\quad y_i = +1 \w^Tx_i+b \leq -1,&amp;\quad y_i = -1\end{cases} 于$||w||^2=w^Tw$最小，是一个凸二次规划(convex quadratic programming)问题，用Lagrange multiplier $\alpha _i \geq 0$得到其对偶问题(dual problem)，可以高效求解。 $Q(\alpha)=\sum \alpha_j - \frac{1}{2}\sum \sum \alpha_i \alpha_j y_i y_j x_i^Tx_j$(1)$\sum \alpha_iy_i=0$(2)$\alpha_i \geq 0$,for all $\alpha_i$$w=\sum \alpha_iy_ix_i$,$b=y_k-\sum \alpha_i y_i y_j x_i^Tx_k$ ,for any $a_k &gt;0$$f(x)=w^Tx+b$详见西瓜书 kernel trick 核函数详见西瓜书 soft margin &amp; solving详见西瓜书]]></content>
      <categories>
        <category>Datamining</category>
      </categories>
      <tags>
        <tag>webDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘-L8 分类]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L8%2F</url>
    <content type="text"><![CDATA[L8 ClassificationKNN分类算法步骤： step.1—初始化距离为最大值step.2—计算未知样本和每个训练样本的距离diststep.3—得到目前K个最临近样本中的最大距离maxdiststep.4—如果dist小于maxdist，则将该训练样本作为K-最近邻样本step.5—重复步骤2、3、4，直到未知样本和所有训练样本的距离都算完step.6—统计K-最近邻样本中每个类标号出现的次数step.7—选择出现频率最大的类标号作为未知样本的类标号 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。 KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比（组合函数）。 KNN的不足 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 对于这个问题，可以采用权值的方法（和该样本距离小的邻居权值大）来改进。 该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。 目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 贝叶斯分类见《模式识别》课程]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘-L7 聚类的应用]]></title>
    <url>%2F2017%2F07%2F26%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L7%2F</url>
    <content type="text"><![CDATA[L7 Applications of Clustering聚类：类间分离，类内集中聚类的类型，根据不同的类型应用不同的算法： well-separated clusters center-based clusters contiguous clusters density-based clusters Property or Conceptual clusters Described by an Objective Function 目标函数 聚类的过程就是maximize或者minimize目标函数的过程 通过枚举所有可能的聚类并用目标函数去计算goodness 通用与否： Hierarchical clustering algorithms针对local object Partitional algorithms通用 从数据中学到参数数据挖掘中 聚类的要求 大规模 不同类型的属性 聚类的形状 初始化的参数尽量少 噪声和极端值 对输入数据的顺序不敏感 高维数据 用户规定的约束 可解释性和可用性 相似性相似性的判断比聚类算法更重要，它表明了对数据的侧重点的不同。 (Dis)similarity measures 相异性度量Jagota defines a dissimilarity measure as a function f(x,y) such that f(x,y) &gt; f(w,z) if and only if x is less similar to y than w is to z data item的折线图 欧式距离 折线图的趋势皮尔逊线性相关Pearson Linear Correlation PLC 相似性的计算 相异性度量 当其他非线性相关时，用PLC则无法来衡量相似性。 Hierarchical Clustering参考层次化的聚类 算法：自底向上，一开始，每个数据点各自为一个类别，然后每一次迭代选取距离最近(Average Linkage\Single Linkage两个最近的点\Complete Linkage两个最远的点)的两个类别，把他们合并，直到最后只剩下一个类别为止，至此一棵树构造完成。 作用： 确定类别数K 利用树，自底向上人工判断类别。直接用k-means等方法无法直接判断出准确的有意义的类别。 缺点： 可能无法分出不同的类，或者需要有特定的cutoff values k-means clustering 和 聚类的质量评估Cluster Quality之前学习过k-means知道，k-means需要给出k以及一开始的random值。不同的k和随机值会导致结果的不同，而且当数据形状不好时该算法分类并不好。 K的问题因此选择k并且利用Cluster Quality Measures评估CQ就有必要了如图，计算Q即每个类别的相异性程度的和要尽量小，即越紧凑。该评估强调了分类后的数据的一致性homogeneity 初始random的问题 对密度分布的处理 density-based clusters该算法需要数据的形状、处理噪音、一次扫描、需要设置密度的阈值Several interesting studies: DBSCAN: Ester, et al. (KDD’96) OPTICS: Ankerst, et al (SIGMOD’99). DENCLUE: Hinneburg &amp; D. Keim (KDD’98) CLIQUE: Agrawal, et al. (SIGMOD’98) DBSCAN核心对象、e邻域、直接密度可达、密度可达、密度相连算法： 扫描整个数据集，找到任意一个核心点，对该核心点进行扩充。扩充的方法是寻找从该核心点出发的所有密度相连的数据点（注意是密度相连）。遍历该核心点的e邻域内的所有核心点（因为边界点是无法扩充的），寻找与这些数据点密度相连的点，直到没有可以扩充的数据点为止。最后聚类成的簇的边界节点都是非核心数据点。之后就是重新扫描数据集（不包括之前寻找到的簇中的任何数据点），寻找没有被聚类的核心点，再重复上面的步骤，对该核心点进行扩充直到数据集中没有新的核心点为止。数据集中没有包含在任何簇中的数据点就构成异常点。 伪代码:所有点都找密度相连，再选出最大的集合，挖掉该集合。重复算法。 算法描述：算法： DBSCAN输入： E——半径MinPts——给定点在E邻域内成为核心对象的最小邻域点数。D——集合。输出： 目标类簇集合方法： Repeat1） 判断输入点是否为核心对象2） 找出核心对象的E邻域中的所有直接密度可达点。Until 所有输入点都判断完毕Repeat针对所有核心对象的E邻域内所有直接密度可达点找到最大密度相连对象集合，中间涉及到一些密度可达对象的合并。Until 所有核心对象的E领域都遍历完毕 弱点：密度的差异数据维度高 对密度和核心对象的确定： 聚类的应用市场调研：用户基数大的时候，选出每个类的典型用户去调研。土地使用：开店铺的选址。根据每个地的属性（人流量、人流种类、消费习惯）聚类，然后去找卖的好的奶茶店在哪种类型的地里，然后去其他相同类型的地里开店。可以用百度API]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魔鬼经济学]]></title>
    <url>%2F2017%2F07%2F26%2F%E9%AD%94%E9%AC%BC%E7%BB%8F%E6%B5%8E%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[第一章 动机与欺骗：学校老师&amp;相扑运动员之间的相似处什么是动机首先明白这本书的逻辑基础。即人类发生行为的动机。动机： 经济动机社会动机道德动机 举个例子： 政府为每一盒烟增加额外的税收——反向的经济动机政府规定公众场合不能吸烟——强力的社会动机政府说犯罪集团通过黑市香烟交易筹资——道德激励 现如今，政府一般通过综合手段来遏制犯罪。 欺骗的几个例子日托例子：幼儿园规定四点家长需要接送孩子。但是总会有迟到的家长，于是经济学家展开一项研究，施行罚款制度。罚款制度的开展，导致迟到的家长增多。之后取消罚款，迟到率并没有回复原因如下： 罚款金额小，比照顾孩子的时间成本低。 罚款行为的经济动机代替了道德动机的罪恶感 停止罚款后，迟到的家长不用罚款且没有罪恶感 同理，献血得到小额奖金反而导致献血者的高尚行为变成交易，反而会影响献血。但是血价格“涨”后，会有人报名献血，甚至做出欺骗行为（假血、偷血、假身份证避开献血额度）。W.C.Fields说过越有价值的事物越容易引发人们进行欺骗动机。可以说，任何人都可能欺骗。上至CEO的内部关联交易，下到随手吃一块饼干而未付款。 芝加哥公立学校系统的教师作弊行为因为该系统的考核影响教师的评级，导致教师也参与到作弊的行为中。观察数据：难题反而作对？多名差生答案一致？…总成绩在这次考核中突然提升之后有下降。确认作弊：让优秀班级和可疑班级同时参与再一次考试，和之前的考核对比。教师的职责是通过以身作则来向学生灌输有益的价值观念。 日本相扑例子日本相扑是一项具有全国性、宗教性的神圣的运动。日本相扑运动员的奖罚机制，使得运动员需要开展巡回赛。这一过程中，也会因为利害关系而发生作弊行为。运动员甚至一个俱乐部心照不宣的作弊很隐晦，通常会是在对方关键比赛中让出胜利，而得胜方下一次会回报一局。这可以通过两人普通比赛中的胜负情况，和关键比赛让胜后的胜负情况来分析，而且关键比赛得胜后，下一局遇到失败率反而大大增加。【这样的作弊行为，会被官方斥为无中生有，人们会不由自主维护这项运动的神圣性】 所以如果相扑手，学校教师，还有日托中心的家长们都会欺骗的话，我们是否可以认为欺骗是一种植根于人性的现象呢？如果是这样的话，我们人类到底有多堕落？ 人类堕落吗？小甜饼例子Paul Feldman年轻时候把小甜饼放到办公室，拿走小甜饼的人通常会在盒子里投入相应的钱。这是白领犯罪的侧面切入口。 爱国情绪激发减少欺骗行为 小公司反而付款率更高【小型社区通常对犯罪者施加较强的社会压力。熟人乡村-陌生人城市】 心情好更诚信 员工喜欢老板和这份工作时更诚信 员工更诚信【中上层领导懂得欺骗才能当上执行官？】 如果说伦理道德代表着一种理想的社会运行模式的话，那么经济学则告诉我们这个世界在现实中是如何运行的，而费尔德曼卖甜饼的故事就恰恰位于道德标准和经济学的交叉点上。 “无论一个人有多么自私，”亚当·斯密写道，“他都会坚守一些原则，去维护别人的利益，让别人感到幸福，虽然他可能并不会从中得到什么！” 第二章 信息：三k党跟房地产经纪人的相识点没有什么比信息更加强大不具体记录书中讲述的三k党组织、章程和行动戴斯森·肯尼迪通过电台节目，将三k党的秘密公之于众。 “三k党的权力就好像政治家、房地产经纪人、股票经纪人的权力一样，在很大程度上来自于他们所掌握的信息。一旦这些信息落入错误的人手上，整个群体的优势就会在瞬间消失。” 二手交易中，新车转手价格不会超过3/4，因为人们会认为车有问题，而推迟一年后再卖，人们对汽车质量的怀疑心理就会减弱。 交易的一方总是会比另一方拥有一些信息优势，这是一种很普遍的现象。用经济学术语来说，这种现象被称为信息不对称 信息不对称这一点，我在一次二手交易中，倒是有所体会。 资本主义所信奉的一条真理就是：一些人（通常是专家）总是比另外一些人 （通常是消费者）知道得更多一些。但到目前为止，各种各样的信息不对称事实上都已经被互联网的发展所削弱。信息是互联网世界的流通货币。作为一种媒介形式，互联网可以迅速地把信息从那些拥有它的人手上传递到那些需要它的人手上。 原本的交易价差为主的盈利会减弱，新的商业模式会形成。互联网缩小了专家与大众之间的距离。 有些时候，跟专家进行面对面交流反而会加大专家和公众之间的信息不对称。因为此时专家很可能会利用他的信息优势让你感觉自己非常愚蠢、低级、卑贱或无知 医生说你必须做手术？是否是真实的为你着想。信息武器利用人的恐惧心理进行攻击。你购买廉价骨灰盒质量是否更差？你不做这个手术是否就不会康复？一辆好车是否比普通车更舒适和安全？这些问题，专家都会从他们的角度，有选择的告诉你或真或假的信息。 第三章 思维陷阱：毒品贩子还和妈妈住一起的原因传统智慧 在他（John Kenneth Galbraith）看来“传统智慧”并不是一个褒义词。“人们经常会把那些便利的，”他写道，“以及与我们的个人利益联系在一起的事物认定为真理，并努力拒绝那些让我们感到不适或跟我们的生活常识格格不入的东西。另一方面，我们又总是会接受那些有助于提升我们自尊的东西。” 思维惯性？因为这些思维的定势，人们往往会反而找证据证明自己原本的观点。 对于记者来说，他们每天都要为自己的新闻报纸和电视节目提供大量的资料和素材，所以他们总是很喜欢关注那些能提供新闻话题的专家。 而这种思维惯性一旦形成，很难被改变。 无论通过怎样的方式，“传统智慧”一旦形成，就很难得到改变 而作者也在书中提到，舆论塑造出“毒贩赚钱”的形象，使得社会对其厌恶，但是实际上，大部分毒贩还是和妈妈住在一起，金字塔尖顶的毒枭才十分富有。 他们都希望能够在这个充满高度竞争的行业里取得成功，而且一旦成为该行业的顶级认识，他们就能发大财，此外还有随之而来的荣耀和权力 这类工作如金融行业、广告行业等。 第四章 犯罪分子都到哪里去了第五章 怎么才能成为完美的父母第六章 名字对孩子的未来有影响吗后记 通往哈佛的两条路]]></content>
      <categories>
        <category>Reading</category>
      </categories>
      <tags>
        <tag>ReadingNote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机与社会学的两个有趣实验]]></title>
    <url>%2F2017%2F04%2F12%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8E%E7%A4%BE%E4%BC%9A%E5%AD%A6%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%9C%89%E8%B6%A3%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[之前看的《大糖帝国》文章（我在Java-并发中提到过），觉得是计算机模拟社会发展的一个有趣实验。昨晚在浏览微博（详见文章Reddit的place作画实验）的时候，发现了Reddit也做了一次有趣的实验。之前的大糖帝国是一次经济发展贫富差距的模拟，这场实验就像是一次人类建造巴别塔的模拟。人类的创造与破坏、竞争与合作、抢占与妥协，最终都在语言沟通，或者通过盘面观察达到了平衡。计算机模拟的实验是一种既定的伪随机的方式，人类的参与让这场实验的不可预测的结果显得格外的有趣。]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 并发]]></title>
    <url>%2F2017%2F04%2F12%2FJava-%E5%B9%B6%E5%8F%91%2F</url>
    <content type="text"><![CDATA[写在前面多线程和并发可以是专门的一本书了，这里简单学习了下，参考阅读《Thinking in Java》“并发”部分。 新类库中的构件在实际应用中应该挺重要的，我看到最后觉得太累了，反而还没看。和新IO简直是一模一样的情况啊。 并发的多面性简单地说，Java的多线程机制就是在一个进程内(系统分配的资源固定)，将CPU的时间切片再分给不同的线程。这样使得控制权的转移对编程人员来说是透明的（或者说是黑盒）。对于多CPU来说，并发操作无疑是高效的。学习并发，也有利于理解构架分布式系统时候用到的消息机制。另外，有函数型编程可以将并发任务彼此隔离。这点需要之后学习过程中接着体会， 基本的线程机制定义任务1234567891011121314151617181920212223242526272829303132333435363738//Thread的start()自动调用Runnable的run()public class Multitask &#123; public static void main(String[] argus) &#123; for (int i = 0; i &lt; 3; i++) &#123; LiftOff liftOff = new LiftOff(); Thread t = new Thread(liftOff); t.start(); &#125; System.out.println("main thread still runs"); &#125;&#125;class LiftOff implements Runnable&#123; protected int countDown = 10; private static int taskCount = 0; private final int id = taskCount++; public LiftOff() &#123; &#125; public LiftOff(int countDown) &#123; this.countDown = countDown; &#125; public String status()&#123; return "#"+id+"("+(countDown&gt;0?countDown:"Liftoff!")+")."; &#125; @Override public void run() &#123; System.out.print("thread"+id+"runs:"); while(countDown--&gt;0)&#123; System.out.print(status()); Thread.yield();//表示暂停当前线程，执行其他线程(包括自身线程) //加入yield()使得每次打印后都会切换线程，不加则由cpu觉得切换的时间 &#125; &#125;&#125;/*main thread still runsthread1runs:thread0runs:thread2runs:#1(9).#0(9).#2(9).#1(8).#2(8).#0(8).#2(7).#1(7).#2(6).#0(7).#2(5).#1(6).#0(6).#2(4).#1(5).#0(5).#2(3).#1(4).#0(4).#2(2).#1(3).#2(1).#0(3).#1(2).#2(Liftoff!).#0(2).#1(1).#0(1).#1(Liftoff!).#0(Liftoff!).*/ 以上主线程和t线程间的调度是由线程调度器自动控制的，如果在多CPU机器上，线程调度器会分发线程给不同处理器。而线程调度器的调度机制是非确定性的，因此输出先后(执行先后)无法保证。 使用Executor在JavaSE5(也就是jdk1.5)的java.util.concurrent.*中执行器(Executor)将管理Thread对象。将上述的main()中代码更改如下：123456789public static void main(String[] argus) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 3; i++) &#123; LiftOff liftOff = new LiftOff(); exec.execute(liftOff); &#125; exec.shutdown(); System.out.println("main thread still runs");&#125; CachedThreadPool会为每个Runnable创建线程，而线程由Executor管理，其中shutdown()方法是防止Executor有新任务提交。另外还有 FixedThreadPool固定了线程池的数量， 和SingleThreadExecutor(相当于FixedThreadPool为1的时候)保证其他进程不会被并发调用。如果向SingleThreadExecutor提交多个任务，这些任务将会排队即序列化任务。将上述的main()中代码更改如下：12345678910111213141516public static void main(String[] argus) &#123; ExecutorService exec = Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 3; i++) &#123; LiftOff liftOff = new LiftOff(); exec.execute(liftOff); &#125; exec.shutdown(); System.out.println("main thread still runs");&#125;/*thread0runs:main thread still runs#0(9).#0(8).#0(7).#0(6).#0(5).#0(4).#0(3).#0(2).#0(1).#0(Liftoff!).thread1runs:#1(9).#1(8).#1(7).#1(6).#1(5).#1(4).#1(3).#1(2).#1(1).#1(Liftoff!).thread2runs:#2(9).#2(8).#2(7).#2(6).#2(5).#2(4).#2(3).#2(2).#2(1).#2(Liftoff!).*/ 从任务中产生返回值12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.util.ArrayList;import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class Multitask &#123; public static void main(String[] argus) &#123; ExecutorService exec = Executors.newSingleThreadExecutor(); ArrayList&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 3; i++) &#123; TaskWithResults task = new TaskWithResults(i); Future&lt;String&gt; result = exec.submit(task);//submin返回Future对象 results.add(result); &#125; for (Future&lt;String&gt; future : results) &#123; if (future.isDone()) &#123; try &#123; System.out.println(future.get()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125; finally &#123; exec.shutdown();//必须有 &#125; &#125; &#125; &#125;&#125;class TaskWithResults implements Callable&lt;String&gt;&#123; private int id; public TaskWithResults(int id) &#123; this.id = id; &#125; @Override public String call() throws Exception &#123; return "result of task "+id; &#125;&#125;/*result of task 0result of task 1result of task 2*/ 休眠将run()中的yield()改为sleep()，使得任务中止执行一定时间12345678910111213141516171819202122232425262728class LiftOff implements Runnable&#123; protected int countDown = 10; private static int taskCount = 0; private final int id = taskCount++; public LiftOff() &#123; &#125; public LiftOff(int countDown) &#123; this.countDown = countDown; &#125; public String status()&#123; return "#"+id+"("+(countDown&gt;0?countDown:"Liftoff!")+")."; &#125; @Override public void run() &#123; try &#123; while(countDown--&gt;0)&#123; System.out.print(status()); //Thread.yield(); //Old style //Thread.sleep(100); //Java SE5/6 style TimeUnit.MILLISECONDS.sleep(100); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace();//异常不能跨线程 &#125; &#125;&#125; 优先级优先级只是决定执行的频率，而一般也不设置优先级。并且不同系统优先级等级分层不同，因此建议使用MAX_PRIORITY、MIN_PRIORITY、NORM_PRIORITY等12345678910111213141516171819202122232425262728293031323334353637383940import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class Multitask &#123; public static void main(String[] argus) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; LiftOff liftOff = new LiftOff(Thread.MIN_PRIORITY); exec.execute(liftOff); &#125; exec.execute(new LiftOff(Thread.MAX_PRIORITY)); exec.shutdown(); &#125;&#125;class LiftOff implements Runnable&#123; protected int countDown = 5; private volatile double d;//发现 volatile 变量的最新值 private int priority; public LiftOff(int priority) &#123; this.priority = priority; &#125; public String toString()&#123;//覆盖 return Thread.currentThread()+":"+countDown; &#125; @Override public void run() &#123; Thread.currentThread().setPriority(priority); while(true) &#123; for (int i = 0; i &lt; 100000; i++) &#123; d += (Math.PI+Math.E)/(double)i; if(i%1000==0)&#123; Thread.yield(); &#125; &#125; System.out.println(this); if (--countDown==0) &#123; return; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132/*Thread[pool-1-thread-6,10,main]:5Thread[pool-1-thread-3,1,main]:5Thread[pool-1-thread-5,1,main]:5Thread[pool-1-thread-1,1,main]:5Thread[pool-1-thread-6,10,main]:4Thread[pool-1-thread-2,1,main]:5Thread[pool-1-thread-4,1,main]:5Thread[pool-1-thread-6,10,main]:3Thread[pool-1-thread-1,1,main]:4Thread[pool-1-thread-5,1,main]:4Thread[pool-1-thread-3,1,main]:4Thread[pool-1-thread-4,1,main]:4Thread[pool-1-thread-2,1,main]:4Thread[pool-1-thread-6,10,main]:2Thread[pool-1-thread-1,1,main]:3Thread[pool-1-thread-5,1,main]:3Thread[pool-1-thread-3,1,main]:3Thread[pool-1-thread-4,1,main]:3Thread[pool-1-thread-6,10,main]:1Thread[pool-1-thread-2,1,main]:3Thread[pool-1-thread-3,1,main]:2Thread[pool-1-thread-5,1,main]:2Thread[pool-1-thread-1,1,main]:2Thread[pool-1-thread-4,1,main]:2Thread[pool-1-thread-2,1,main]:2Thread[pool-1-thread-3,1,main]:1Thread[pool-1-thread-1,1,main]:1Thread[pool-1-thread-5,1,main]:1Thread[pool-1-thread-4,1,main]:1Thread[pool-1-thread-2,1,main]:1*/ 让步yield()表示建议暂停当前线程，执行其他相同优先级的其他线程。 后台线程非后台线程结束后，会杀死所有后台线程123456789101112131415161718192021222324252627import java.util.concurrent.TimeUnit;public class Multitask &#123; public static void main(String[] argus) &#123; Thread t = new Thread(new ADaemon()); t.setDaemon(true); t.start(); System.out.println("finish"); &#125;&#125;class ADaemon implements Runnable&#123; @Override public void run() &#123; try &#123; System.out.println("Starting ADaemon"); TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; System.out.println("always run?"); &#125; &#125;&#125;/*finishStarting ADaemon*/ 同时，后台线程创建的线程仍然是后台线程 编码的变体看源码Thread类也是实现Runnable接口。所以实现线程有这么几种方式： 实现Runnable接口 实现其中的run() 传入Thread对象，执行start()。 自管理的Runnable。 类中变量Thread t = new Thread(this)，并在构造函数中执行t.start() 直接继承Thread类 实现run() 构造函数中执行start() 用内部类隐藏线程代码 实现一个内部类，该内部类继承Thread 实现该内部类的run()和构造函数中执行start() 该类有一个变量为这个内部类private InnerClassName test; 在类的构造函数中new这个内部类test = new InnerClassName() Executor管理的线程池 实现Runnable接口 传入到Executor中 加入一个线程 Join()如下，Joiner线程需要等到Sleeper线程结束或者中断sleeper.interrupt()，才会接着执行1234567891011121314151617class Joiner extends Thread&#123; private Thread sleeper; public Joiner(String name, Thread sleeper) &#123; super(name); this.sleeper = sleeper; start(); &#125; @Override public void run() &#123; try &#123; sleeper.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(getName()+" join completed"); &#125;&#125; 捕获异常JavaSE5前需要用线程组。现在给线程附上一个异常处理前就可以捕获异常了。当异常传到线程外时，已经无法被捕捉(unknown source)，即使外部再调用try-catch也没用。但是用123456789Thread t = new Thread(new Runnable类);t.setUncaughtExceptionHandler( new UncaughtExceptionHandler() &#123; @Override public void uncaughtException(Thread t, Throwable e) &#123; //do something &#125; &#125;//匿名内部类); 也可以统一给Thread设置一个静态域1Thread.setUncaughtExceptionHandler(new MyUncaughtExceptionHandler()) 共享受限资源解决共享资源竞争： synchronized关键字对资源加锁。第一个访问该资源的任务（或者说是线程）锁定该资源。Java中，当任务要执行synchronized关键字保护的代码片段时，会检查锁是否可用，然后获得锁。通常将域设置为private，而能访问该域的方法都用synchronized。synchronized则是在类的范围内防止对static数据的并发访问。 Lock对象JavaSE5中还有Lock对象的显式互斥机制。人工加锁解锁比之synchronized，Lock更灵活，但是代码更多。一般用前者。但是当有特殊要求synchronized获取锁失败、需要给获取锁加一定时间期限等，用Lock类自定义显然更理想。 1234567891011private Lock lock = new ReentrantLock();public int next()&#123; lock.lock; try&#123; currValue++; currValue++; return currValue; &#125; finally&#123; lock.unlock(); &#125;&#125; volatile关键字volatile获取原子性和可视性，避免编译器的优化。Java中的基本类型的操作是原子性的，但是读写long、double等会出现分成两个字节来处理的情况，不是原子性的。并且注意到，Java递增操作也不是原子性的。 原子类JavaSE5引入了一些原子性变量类，主要用于性能调优。Atomic类主要用来构建java.util.concurrent中的类。详情见书 临界区想要只对部分代码（临界区）进行控制，可以用同步控制块来处理 1234synchronized(指定某个对象如syncObject)&#123;//为进入这段代码，需要获得该对象syncObject的锁//最好是使用其方法正在被调用的当前对象即synchronized(this)&#125; 在其他对象上同步即如下代码传入其他对象。为了起到互斥作用，所有相关的任务应该都是在这个对象上完成的，否则达不到目的 1234private Object syncObject = new Object();synchronized(syncObject)&#123;//something&#125; 线程本地存储 12345public static ThreadLocal&lt;Integer&gt; value = new ThreadLocal&lt;&gt;()&#123; protected synchronized Integer initialValue() &#123; return 3;//为每个线程都返回值 &#125;&#125;; 终结任务exec.awaitTermination(250,TimeUnit.MILLISECONDS)若所有任务都在超时前结束，返回true sleep的阻塞wait() 或者sleep()会使得线程进入阻塞状态，在阻塞时终结线程，需要执行中断操作。 Thread类有interrupt()方法，执行后会抛出InterruptedException异常 Thread.interrupted()来做run()中的循环条件 Executor调用shutdownNow()会中断所有的任务 Executor调用submit()得到的返回值泛型Future&lt;?&gt; tmp，执行tmp.cancle(true)将会中断特定的线程 io的阻塞注意到之前IO阻塞和synchronized造成的阻塞无法中断解决： 在Executor调用shutdownNow()后，其他IO的阻塞需要用System.in.close()和ServerSocket的close()方法手动关掉发生阻塞的底层资源 对于NIO的channel类，则 Executor调用shutdownNow()会中断所有的任务 Executor调用submit()得到的返回值泛型Future&lt;?&gt; tmp，执行tmp.cancle(true)将会中断特定的线程 仍旧可以手动关掉资源close() 被互斥所阻塞为解决synchronized造成的阻塞无法中断，JavaSE5添加了特性，ReentrantLock上阻塞的任务可以被中断即在Runnable中阻塞1234private Lock lock = new ReentranLock();lock.lock();//something//lock.unlock();//注释后造成阻塞 可以用Thread的interrupt()中断 检查中断假如并没有阻塞，导致interrupt()后继续run()考虑到InterruptedException异常和Thread.interrupted()都会在调用后清楚中断的状态，因此我们可以在run()中这么处理：12345public void run()&#123; while(!Thread.interrupted())&#123; //do something &#125;&#125; 线程之间的协作wait() notifyAll() wait()期间锁是释放的 可以通过notify()、notifyAll() 或者wait()时间到期，从wait中恢复执行 wait()、notify()、notifyAll()只能在同步控制方法或同步控制块中调用12345678910111213public void run()&#123; while(Thread.interrupted())&#123; //something /* synchronized的方法中 flag_A = true; notifyAll(); */ /* synchronized的方法中 while(flag_B == false) wait(); */ &#125;&#125; 实际情况中，可能线程1还未wait(),线程2已经发出notify()，导致错失的信号 notify() notifyAll()无论notify() notifyAll()，都只是唤醒等待同一个锁的线程。 生产者消费者 使用显式的Lock和Condition对象类似于wait() notify()部分1234567891011121314151617181920212223public void run()&#123; while(Thread.interrupted())&#123; /* 已经不用synchronized关键词,某个方法中： lock.lock(); try&#123; flag_A = true; condition.signAll();//需要拥有这个锁 &#125; finally&#123; lock.unlock(); &#125; */ /* 已经不用synchronized关键词,某个方法中： lock.lock(); try&#123; while(flag_B == false) wait(); &#125; finally&#123; lock.unlock(); &#125; */ &#125;&#125; 生产者消费者队列wait() notify()方法很低级，用更高级抽象的同步队列来解决。java.util.concurrent.BlockingQueueLinkedBlockingQueueArrayBlockingQueue消费者任务试图从队列里获取对象，如果此时为空，会挂起消费者，等到有元素时再恢复。1234private BlockingQueue&lt;类&gt; rockets;rockets = 某些队列rockets.put(类的对象)rockets.take()//取出来 任务间使用管道进行输入输出管道输入，输出时候空会自动阻塞，值得一提的是，不同于普通的I/O，管道是可中断的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.io.IOException;import java.io.PipedReader;import java.io.PipedWriter;import java.util.Random;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class Multitask &#123; public static void main(String[] argus) &#123; Sender sender = new Sender(); Receiver receiver = new Receiver(sender); ExecutorService exec = Executors.newCachedThreadPool(); exec.execute(sender); exec.execute(receiver); try &#123; TimeUnit.SECONDS.sleep(4); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; exec.shutdownNow(); &#125;&#125;class Sender implements Runnable&#123; private Random random = new Random(47); private PipedWriter out = new PipedWriter(); public PipedWriter getPipeWriter()&#123;return out;&#125; @Override public void run() &#123; try &#123; while (true) &#123; for (char c = 'A'; c &lt;= 'Z'; c++) &#123; out.write(c); TimeUnit.MILLISECONDS.sleep(random.nextInt(500)); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125;class Receiver implements Runnable&#123; private PipedReader in; public Receiver(Sender sender) &#123; try &#123; in = new PipedReader(sender.getPipeWriter()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void run() &#123; try &#123; while (true) &#123; System.out.println("Read: "+(char)in.read()+","); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 死锁书中对哲学家就餐问题进行了模拟。同时指出死锁四个必要条件： 互斥条件。存在资源不能共享 占有某资源且等待另一资源 资源不能被抢占 存在循环等待。A等待B，B等待A 仿真书中提到的几种对现实场景的仿真，如饭店服务、银行柜台服务等。这里不具体展开。之前读过一篇经济学的文章，比较有意思，叫《大糖帝国》。是说在一个Sugarscape的棋盘上，一些人根据固定的生存法则自主的生存，最终一定会是贫富分化，满足二八定律。但是文章的结论同时指出，对于致富，“天赋论”和“出身决定一切”都不是决定性的。那么什么是决定性的？然而什么也不是，这个过程是复杂的。 有兴趣可以再进一步实现一下，假设每个单元(姑且这么称号，毕竟实际生活还有公司法人、自然人、乃至国家等各种经济体)就像《自私的基因》一样惟利是图（然而现实中有无数献身公益事业的人），而我们指定策略（如税收等财富再分配手段）来进行干预。但是这过程又需要每个单元进行智能的博弈。感觉会很有趣，留坑。 新类库中的构件待阅读、实验。 JavaSE5的java.util.concurrent引入了很多好用的新类。 CountDownLatch CyclicBarrier DelayQueue PriorityBlockingQueue ScheduledExecutor Semaphore Exchanger 性能调优待阅读、实验。 介绍不同方式的性能比较。Vector 和 Hashtable TreeSet 线程安全的免锁容器 活动对象待阅读、实验。 多线程模型的替换方式。基于代理 总结及进阶读物 Java Concurrency in Practice Concurrent Programming in Java The Java Language Specifacation]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Thinking in Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘 L6 聚类]]></title>
    <url>%2F2017%2F04%2F12%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L6%2F</url>
    <content type="text"><![CDATA[L6 Clustering特征： 特征提取 feature Extraction 特征向量 特征空间 Metric 距离：在使用欧氏距离的时候，二维是圆，三维是球..如此，用它来聚类的标准会倾向于圆/球等。 因此当数据是聚类的结果并不好。 Euclidean space欧氏空间 Position: x,y Similarity: $=\sum_ix_iy_i$ 内乘 $$ distance: $d(x,y)=|x-y|=\sqrt()=$ Induced norm导出范数|x-y| Similarity-Distance:$d(x,y)^2==+-2$cluster 聚类聚类是在无监督的情况下得到自然特征的方法。本课介绍k-means 和 k-medians k-means算法： 假设要分为3类，随机在样本中选择3个点 每个样本计算到3个点的距离，把样本归类到最近的点所在的类 更新中心点：每个类的各个维度$x_i,y_i,z_i…$的平均值 重复2步骤、3步骤，直到中心点的变化小于某个阈值，结束算法 优点： 简单好理解 样本自动归类缺点： 手动选择类别数 所有的样本都会被强制分类 个别异常点太远了会导致平均值偏离，中心点偏离k-medians因为k-means异常点的敏感性，提出该改进算法算法： 假设要分为3类，随机在样本中选择3个点 每个样本计算到3个点的距离，把样本归类到最近的点所在的类 更新中心点：每个类的每个维度如$x_i$排序后的中位值，作为中心点该维度的坐标 重复2步骤、3步骤，直到中心点的变化小于某个阈值，结束算法 优点： 解决异常点问题缺点：大数据时候排序耗时（解决：sampling 从样本中随机选择小数量的sample） Appendix推荐一篇把聚类用于图像颜色更换的论文：Palette-based Photo Recoloring]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘 L4L5 网页排序]]></title>
    <url>%2F2017%2F04%2F12%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L4L5%2F</url>
    <content type="text"><![CDATA[L4 Ranking AggregationSocial Choice Theory社会选择理论是关于投票的理论 我们如何做决定： 硬币 Dictatorship独裁 Democracy (Majority rule)大多数 还要考虑群体的异质偏好heterogeneous preferences 集体智慧现在的民主方式其实是默认了一个前提，“集体智慧”。 Cognition 群体决策会比一部分专家更高效和客观 Coordination 代表了整体的文化认可度 Cooperation 避免胁迫，更自由 但其实集体智慧有很多因素要考虑。不是所有人都能做正确明智的选择： 差异化 盲从 意见分散 争论 主观、意见分散、集权、跟从都可能导致失败处于对不同人的意见的综合考虑，实际生活中就有了各种应用： 选举 民意调查 特定的选举规则（如美国的赢家通吃） 搜索排行 Rule Majority rule 多数 Condorcet paradox ：康多塞悖论，投票的结果和投票的顺序有关 Borda-Rule 博尔达计分法：每个人给每个方案都打分，再统计其他变形： Weighted Borda-Rule With relevant scores available早期算法： Min, Max and Average model[Fox and Shaw,1995] Algorithm Final score CombMin minimum of individual relevance scores CombMed median of individual relevance scores CombMax maximum of individual relevance scores CombSum sum of individual relevance scores CombANZ CombSum / num non-zero relevance scores CombMNZ CombSum * num non-zero relevance scores Linear Combination Model[Bartell 1995]给每个数据加权 Logistic Regression Model 以上算法在TREC会议上多有应用。 L5 Web Structure Mining介绍网络结构： 图 特征： 大 未知 动态 因此需要考虑实际的关注点、计算能力、内存等情况来构造网络图。为了构造这张图，先定义以下几个函数关系： Back_url(the_url)找出所有指向本url的url 一定程度上反应重要性 苦难 利用搜索引擎link:url Shortest_Path(url,url2)最短路径 Maximal_Clique(url)最大团，类似于找url的闭包 In_Degree(url)入度 Web Graph MiningFan：Back_url流行程度真粉？特殊情况：google.com等 PageRank的计算能够表明网页的流行程度。其中T是指向A的网页，而C(T)是T指向网页的总数$$PR(A)=(1-d)+d*(\frac{PR(T_1)}{C(T_1)}+\frac{PR(T_2)}{C(T_2)}+…\frac{PR(T_n)}{C(T_n)})$$举个例子： PR(a)=1, PR(b)=1, PR(c) =1 Web Community给定一些网页，找他们中的密集连接在一起的Community 完全子图 完全双向子图 附：数据堂:出售数据相关数据集、算法网站http://webla.sourceforge.net/javadocs/pt/tumba/links/WebGraph.htmlhttp://introcs.cs.princeton.edu/java/45graph/Digraph.java.htmlhttp://www.cs.ucsb.edu/~kris/Research/agl/doc/agl2/Digraph.html]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224d Pset 1]]></title>
    <url>%2F2017%2F04%2F04%2FCS224d-Pset-1%2F</url>
    <content type="text"><![CDATA[根据题意， $h=sigmoid(xw_1+b_1)$ $\widehat y=softmax(hw_2+b_2)$ m维度x输入以如下的矩阵形式，每行为一个样例 $$\begin{bmatrix}x_{D1}\ x{D2}\ …\ x{Dm}\x{D1}\ x{D2}\ …\ x{Dm}\x{D1}\ x{D2}\ …\ x{Dm}\.\.\x{D1}\ x{D2}\ …\ x{D_m}\\end{bmatrix}$$ 所以根据上述网络图加入有样例数N，隐藏层$D_h=h$，输出层y有$D_y=n$维输入X矩阵 NmW1矩阵mhW2矩阵hn最终得到Nn的矩阵输出 Softmax(10 points)(a)求证softmax输入常数c不影响结果。$$(softmax ( \mathbf{x} + c ))_i = \frac{\exp (\mathbf{x}i + c ) }{\sum{j=1}^{dim( \mathbf{x})}{\exp(\mathbf{x}_j+c )}} = \frac{\exp(c )\exp ( xi)}{\exp ( c )\sum{j=1}^{dim(x )}\exp( x_j)} = \frac{\exp ( xi)}{\sum{j=1}^{dim(x )}\exp( x_j)} = ( softmax ( \mathbf{x}) )_i$$ (b)实现softmax。见GithubNeural Network Basics (30 points)(a)推导sigmoid函数的导数。$$σ′(x)=σ(x)(1−σ(x))$$ (b)交叉熵的导数首先，相比方差代价函数，交叉熵代价函数用于sigmoid会使得梯度更新更明显。softmax:$$softmax(x_i)=\frac{e^{x_i}}{\sum_k^m e^{x_k}}$$softmax求导:当$j = i$：$$\frac{\partial S_j}{\partial x_i}=\frac{\partial }{\partial x_i}(\frac{e^{x_j}}{\sum_k^m e^{x_k}})\=\frac{\partial }{\partial x_i}(\frac{e^{x_j}}{e^{x_1}+e^{x_2}…+e^{x_k}})\=\frac{e^{x_i}({e^{x_1}+e^{x_2}…+e^{x_k}})-e^{x_i}e^{x_i}}{(e^{x_1}+e^{x_2}…+e^{x_k})^2}\=S_i-S_i^2\=S_i(1-S_i)$$当$j \neq i$：$$\frac{\partial S_j}{\partial x_i}=\frac{\partial }{\partial x_i}(\frac{e^{x_i}}{\sum_k^m e^{x_k}})\=\frac{\partial }{\partial x_i}(\frac{e^{x_j}}{e^{x_1}+e^{x_2}…+e^{x_k}})\=\frac{0({e^{x_1}+e^{x_2}…+e^{x_k}})-e^{x_j}e^{x_i}}{(e^{x_1}+e^{x_2}…+e^{x_k})^2}\=-S_jS_i\$$ 所以$$\frac{\partial S_j}{\partial x_i} =\begin{cases}S_i(1 – S_i),&amp;\quad i = j \-S_i S_j,&amp;\quad i \neq j\end{cases}$$ CE(Cross-entropy):$$CE(y,\widehat y)=-\sum_k y_k log({\widehat y_k})$$$$其中 \widehat y_k = sigmoid(x_k)$$求导：$$\begin{split}\frac{\partial CE(y,\widehat y)}{\partial x_i}&amp;=-\sum_k y_k\frac{\partial \log \widehat y_k}{\partial x_i} \&amp;=-\sum_ky_k\frac{1}{\widehat y_k}\frac{\partial \widehat y_k}{\partial x_i} \\end{split} $$$$其中\sum_ky_k\frac{1}{\widehat y_k}\frac{\partial \widehat y_k}{\partial x_i} =\begin{cases}y_i(1-\widehat yi),&amp;\quad k = i \\sum{k\neq i}y_k\frac{1}{\widehat y_k}({-\widehat y_k \widehat y_i}) ,&amp;\quad k \neq i\end{cases}$$ $$\begin{split}所以带入&amp;=-y_i(1-\widehat yi)+\sum{k\neq i}y_k({\widehat y_i}) \&amp;=-y_i+{y_i \widehat yi+\sum{k\neq i}y_k({\widehat y_i})} \ &amp;={\widehat y_i\left(\sum_ky_k\right)}-y_i \&amp;=\widehat y_i-y_i\end{split} $$其中因为有onehot，$\sum_ky_k=1,yi=1,y{i \neq k}=0$ (c)推导单隐层神经网络的梯度根据题意，$h=sigmoid(xw_1+b_1)$$\widehat y=softmax(hw_2+b_2)$令：$z_1=xw_1+b_1$$z_2=hw_2+b_2$ 该网络cost function的导数$$\begin{split}\frac{\partial CE}{\partial x}&amp;= \frac{\partial CE}{\partial z_2}\frac{\partial z_2}{\partial h}\frac{\partial h}{\partial z_1}\frac{\partial z_1}{\partial x}\&amp;= (\widehat y-y)(w_2^T)(sigmoid^{‘}(z_1))(w_1^T)\&amp;= (\widehat y-y)(w_2^T)(sigmoid(z_1)(1−sigmoid(z_1)))(w_1^T)\&amp;= (\widehat y-y)(w_2^T)(sigmoid(xw_1+b_1)(1−sigmoid(xw_1+b_1)))(w_1^T)\end{split} $$ (d)神经网络的参数数量计算输入$D_x=m$，输出$D_y=n$再加上h个$b_1和D_y个b_2$共有参数$$(D_x+1)h+(h+1)D_y$$ (e)sigmoid的激活函数和梯度的代码完成q2_sigmoid.py (f)梯度检查代码完成q2_gradcheck.py (g)神经网络代码完成q2 neural.py word2vec(40 points + 5 bonus)(a)待 (b)待 (c)待 (d)待 (e)q3_word2vec.py (f)q3_sgd.py (g)测试运行python q3 run.py (h)在q3_word2vec.py中实现CBOW 情感分析(a)q4_softmaxreg.py (b)解释当分类语料少于三句时为什么要引入正则化（实际上在大多数机器学习任务都这样） (c)q4 sentiment.py (d)绘图 AppendixPset1 tutorialPset1 assignment1Pset1 solutions CodePset1 solutions斯坦福cs224d 大作业测验1与解答 1234567891011121314151617import numpy as npif __name__ == "__main__":## 实验说明axis是tuple作用是对某维度的投影，max则是和投影平行最大的面## 二维数组 0 对x的投影 最大行； 1对y投影最大列## keepdims值为True则是保持维度 y = np.array([[1,2], [3,4]]) ## print len(x.shape) print np.max(y,axis=0,keepdims=True) print np.max(y,axis=0,keepdims=False) print np.max(y,axis=1,keepdims=True) x = np.array([[[1,2], [3,4]], [[5,6], [7,8]]]) print np.max(x,axis=(0,1),keepdims=True)]]></content>
      <categories>
        <category>NLP/CS224d</category>
      </categories>
      <tags>
        <tag>CS224d</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java-IO系统笔记]]></title>
    <url>%2F2017%2F04%2F03%2FJava-IO%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[写在前面之前因为忽然需要文件处理，查了下后发现觉得IO类结构不是很清晰。在阅读《Thinking in Java》关于这部分的介绍后，特在此记录。需要简单的文件读写请直接阅读典型输入使用方式和典型输出使用方式部分。 输入InputStream类InputStream是用来表示从不同数据源产生输入的类，它有以下子类： ByteArrayInputStream：从内存缓冲区 StringBufferInputStream：将String转为InputStream FileInputStream：文件 PipedInputStream：管道化 SequenceInputStream：多个InputStream转为一个InputStream FilterInputStream：作为装饰器的接口，为其他InputStream类提供有用功能。有以下子类： DataInputStream：按照可移植方式从流中读取基本类型 BufferedInputStream ：缓冲区 LineNumberInputStream ：跟踪行号，getLineNumber()等方法 PushbackInputStream ：通常作为编译器的扫描器，能将读到的最后一个字符回退T InputStreamReader类 适配器简单地说，InputStream和Reader的不同在于前者是字节，后者是字符操作。因此，字节到字符间的转换需要适配器。子类： FileReader123new FileReader("eaxmple.txt") //查看源码，发现其实相当于，使用的是默认encodernew InputStreamReader(new FileInputStream("eaxmple.txt")) Reader类子类： FilterReader PushbackReader BufferedReader LineNumberReader 典型输入使用方式字符文本12345678910111213File file = new File("example.txt");//最好检验存在if (file.exists()) &#123; try &#123; FileInputStream stream = new FileInputStream(file);//InputSteam InputStreamReader adapter = new InputStreamReader(stream，"GBK");//适配器 BufferedReader reader = new BufferedReader(adapter);//Reader String tmp = reader.readLine();//读到内存的例子 reader.flush();//此处可省略 reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 二进制文件123456789File file = new File("example.txt");if (file.exists()) &#123; try &#123; FileInputStream stream = new FileInputStream(file); BufferedInputStream buffer = new BufferedInputStream(stream); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 在reader.readLine()等用完后，reader.flush()是将缓冲区的内容进行实际的IO操作，如果不需要多次flush刷新的话，可以省略。因为最后reader.close()时会自动调用flush方法。 FileInputStream可以传File，也可以传入String来构造。根据源码，传入String后仍旧是new File对象再构造FileInputStream。 输出OutputStream类子类： ByteArrayOutputStream：放到在内存创建的缓冲区中 FileOutputStream：写到文件 PipedOutputStream：管道 FilterOutputStream：装饰器，为其他OutputStream提供有用功能 DataOutputStream:按照可移植方式向流中写入基本类型 PrintStream:产生格式化输出。内部实现用的是BufferedWriter，可以设置autoFlush BufferedOutputStream：使用缓冲区 OutputStreamWriter类 适配器继承Writer类，构造器接受OutputStream对象子类： FileWriter123new FileWriter("eaxmple.txt") //查看源码，发现其实相当于，使用的是默认encodernew OutputStreamWriter(new FileOutputStream("eaxmple.txt")) Writer类子类： FilterWriter BufferedWriter PrintWriter：格式化 典型输出使用方式1234567891011121314//字符文件File file = new File("example.txt");if (file.exists()) &#123; try &#123; FileOutputStream stream = new FileOutputStream(file);//OutputSteam OutputStreamWriter adapter = new OutputStreamWriter(stream，"GBK");//适配器 BufferedWriter writer = new BufferedWriter(adapter);//Reader writer.write("test"); writer.flush();//此处可省略 writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 在reader.readLine()等用完后，reader.flush()是将缓冲区的内容进行实际的IO操作，如果不需要多次flush刷新的话，可以省略。因为最后reader.close()时会自动调用flush方法。 PrintWriter的构造器除了和Writer子类一样能接受Writer对象，还接受OutputStream作为参数来构造。Java SE5之后还增加了一个构造器：1234567File file = new File("example.txt");new PrintWriter(file)//查看源码，发现其实相当于如下过程。只是为了少写点装饰过程的代码。new PrintWriter( new BufferedWriter( new OutputStreamWriter( new FileOutputStream(file))) RandomAccessFile类似C语言中的fopen()，在nio中有新的处理方式，暂不详细看这个类。 文件读写实用工具装饰器使得文件读写变得麻烦，PrintWriter简化了输出的过程。而自己也可以封装一些针对Text等常用文件的操作。 虽然PrintWriter可以对数据格式化，便于阅读。但是数据的存储还是建议用DataOutputStream写入，DataInputStream恢复：12345678910111213//存储数据：写入文件File file = new File("example.txt");if (file.exists()) &#123; try &#123; FileOutputStream stream = new FileOutputStream(file); BufferedOutputStream buffer = new BufferedOutputStream(stream); DataOutputStream dataOutputStream = new DataOutputStream(buffer); dataOutputStream.writeUTF("That was pi"); dataOutputStream.writeDouble(3.14159); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 12345678910111213//恢复数据：从文件读出File file = new File("example.txt");if (file.exists()) &#123; try &#123; FileInputStream stream = new FileInputStream(file); BufferedInputStream buffer = new BufferedInputStream(stream); DataInputStream dataInputStream = new DataInputStream(buffer); System.out.println(dataInputStream.readUTF()); System.out.println(dataInputStream.readDouble()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;&#125; 标准IO System.in:public final static InputStream in 需要装饰InputStream-&gt;InputStreamReader-&gt;BufferedReader System.out:public final static PrintStream out PrintStream是OutputStream。而PrintWriter可以接受OutputStream。所以可以PrintStream-&gt;PrintWriter System.err:public final static PrintStream err 标准IO的重定向 System.setIn(文件输入如BufferedInputStream到标准输入) System.setOut(标准输出定向到文件如PrintStream) System.setErr(标准输出定向到文件如PrintStream) 进程控制12Process process = new ProcessBuilder(command_str.split(" ")).start();process.getInputStream()得到一个process标准输出的InputStream 新IOjava.nio.*包引入新的io库，并重新实现了java.io.*待续 对象序列化Java对对象的序列化和反序列化待续 压缩 FilterInputStream：InputStream的子类，装饰器的接口 CheckedInputStream：为InputStream计算CheckSum InflaterInputStream：压缩类的基类 ZipInputStream：Zip压缩 GZIPInputStream：GZIP压缩 FilterOutputStream：装饰器 CheckedOutputStream：为OutputStream计算CheckSum DeflaterOutputStream：解压缩的基类 ZipOutputStream:解压缩Zip GZIPOutputStream:解压缩GZIP GZIP1234567//字节流 输入File file = new File("example.gz");FileInputStream stream = new FileInputStream(file);GZIPInputStream gzip = new GZIPInputStream(stream);InputStreamReader adapter = new InputStreamReader(gzip);BufferedReader reader = new BufferedReader(adapter);String tmp = reader.readLine(); 123456//字符流 输出File file = new File("example.gz");FileOutputStream stream = new FileOutputStream(file);GZIPOutputStream gzip = new GZIPOutputStream(stream);BufferedOutputStream writer = new BufferedOutputStream(adapter);writer.write("this is a test"); ZIPJava的Zip库更全面123456789101112131415161718//压缩File file = new File("example.zip");FileOutputStream stream = new FileOutputStream(file);CheckedOutputStream checksum = new CheckedOutputStream(stream, new Adler32());//加入校验码Adler32(),或者CRC32()等ZipOutputStream zip = new ZipOutputStream(checksum);//Zip输出流BufferedOutputStream buffer = new BufferedOutputStream(zip);zip.setComment("a test");//加入评论等ArrayList&lt;String&gt; filenames = new ArrayList&lt;&gt;();filenames.add("example.txt");//Zip文件中的子文件或者目录filename为"data.txt"或者"/directory"for (String filename : filenames) &#123; zip.putNextEntry(new ZipEntry(filename)); //此时文件的buffer里指针在新的ZipEntry的起始位置，可以写入 int byteData = -1; buffer.write(byteData); buffer.flush();&#125;buffer.close(); 1234567891011121314//解压缩File file = new File("example.zip");FileInputStream stream = new FileInputStream(file);CheckedInputStream checksum = new CheckedInputStream(stream, new Adler32());ZipInputStream zip = new ZipInputStream(checksum);BufferedInputStream buffer = new BufferedInputStream(zip);ZipEntry zipEntry;while(zipEntry = = zip.getNextEntry() != null)&#123;//getNextEntry()会使得指针调到下一个ZipEntry的起始位置 int byteData; while(byteData = buffer.read() != null) System.out.write(x);&#125;System.out.print(checksum.getChecksum().getValue());buffer.close(); 对于读Zip的操作，可以用ZipFile对象读文件，再用entries()方法来向ZipEntries返回枚举 Zip格式也被用于Java档案文件JAR(Java ARchive) XMLXML是一种更为通用的方法。java本身有java.xml.*类库。书中采用XOM库。本质是XML文件的解析和编写过程，对应为程序的读写。root - child结构1234567891011&lt;?xml version="1.0" encoding="ISO-8859-1"?&gt;&lt;people&gt; &lt;person&gt; &lt;first&gt;Dr. Bunsen&lt;/first&gt; &lt;last&gt;Honeydew&lt;/last&gt; &lt;/person&gt; &lt;person&gt; &lt;first&gt;Gonzo&lt;/first&gt; &lt;last&gt;Res&lt;/last&gt; &lt;/person&gt;&lt;/people&gt; PreferencesPreferences API是一个键值对集合，通常用于创建以类名命名的单一节点。如：123456789101112131415161718public class PreferencesDemo&#123; public static void main(String[] args)&#123; Preferences prefs = Preferences.userNodeForPackage(PreferencesDemo.class);//用户 //PreferencesDemo.class在此处作为标识节点 //Preferences.systemNodeForPackage()最好用于通用配置 prefs.put("Location","shenzhen"); prefs.putInt("Companions",4); for(String key:prefs.keys())&#123; print(key+":"+prefs.get(key,null));//put default value &#125; &#125;&#125;/*output:Location:shenzhenCompanions:4*/]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Thinking in Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络数据挖掘 L1-L3 Indexer&Search]]></title>
    <url>%2F2017%2F03%2F28%2F%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-L1-L3-Indexer-Search%2F</url>
    <content type="text"><![CDATA[L1 Instruction略略略 L2 Architecture and Spiders网页、网站架构、搜索引擎等 homework: Spider L3 Indexer and SearchIndexer Tokenizer Tokenization Stopping (a, the, this, that, of, etc.Stop word list) Stemmer词干器（Porter stemming algorithm） Root: fish Words from the root: fishing, fished, fisher Lemmatization词形还原（BE for am, is, are, was, were, been） Spell Checking or Correction Example: s1=“war”, s2=“peace” war&gt;par&gt;pear&gt;peac&gt;peace Operations are: type3, type1, type3,type1 Edit distance=4 Token Sequence：Sequence of (Modified token, Document ID) pairs. sort：Sort by terms And then docID Dictionary and Postings：频率和doc list Search Boolean Model Implementation Three basic operations: AND, OR, NOT Construct a binary tree to represent query Process the input lists bottom-up 优点：精确查询、实现简单 缺点：文档权重一致、用户需要很好的Boolean query Ranked Retrieval Model Jaccard系数：存在Jaccard系数word权重相等的问题 TF&amp;IDF Term frequency tf：term在document中出现越多，该文档和该词越相关 Document frequency df：term在越多document出现，term的信息量越少 Inverse document frequency idf：$idf(t)=log_{10}(N/df(t))$，N是document的总量 Tf-idf:$tf.idf(t, d) = tf(t, d) * idf(t)$ Final Rank: $R(Q,d)=\sum_{t\in Q}tf.idf(t,d)$ Vector Space Model向量空间模型VSM 把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。 于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。 Document = {term1, term2, …… ,term N} 计算R(Q,d)：Document Vector = {weight1, weight2, …… ,weight N} 同样我们把查询语句看作一个简单的文档，也用向量来表示。 Query = {term1, term 2, …… , term N} 计算R(Q,d)：Query Vector = {weight1, weight2, …… , weight N} 我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。 两个向量之间的夹角越小，相关性越大。所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。 正则化并计算cos： Probabilistic Model * Evaluation Precision：The fraction of retrieved documents that are relevant Recall：The fraction of relevant documents that are retrieved homework12345678910111213141516171819202122232425262728293031323334353637383940414243444546//倒排文件索引(Inverted File Index)import java.io.*;import java.util.*;public class Main&#123; public static void main(String[] args)&#123; Main test = new Main(); test.MyInvertedFileBuilde(); &#125; private void MyInvertedFileBuilde()&#123; File dir = new File("documents"); File[] files = dir.listFiles(); LinkedHashMap&lt;Integer, String&gt; src = new LinkedHashMap&lt;&gt;(); LinkedHashMap&lt;String, LinkedHashSet&lt;Integer&gt;&gt; invertedFile = new LinkedHashMap&lt;&gt;(); for (int id = 0; id &lt; files.length; id++) &#123; File file = files[id]; String lineTxt = new String(); if(file.isFile())&#123; try &#123; InputStreamReader read = new InputStreamReader(new FileInputStream(file),"GBK"); BufferedReader bufferedReader = new BufferedReader(read); String tmp = null; while((tmp = bufferedReader.readLine()) != null)&#123; lineTxt = lineTxt + tmp; &#125; src.put(id, lineTxt);//files ID map String[] words = lineTxt.split(" "); for (String word : words) &#123; if(!invertedFile.containsKey(word))&#123; invertedFile.put(word,new LinkedHashSet&lt;Integer&gt;()); invertedFile.get(word).add(id); &#125;else &#123; invertedFile.get(word).add(id); // words inveredFile &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; for (String word : invertedFile.keySet()) &#123; System.out.println(word+" "+invertedFile.get(word)); &#125; &#125;&#125;]]></content>
      <categories>
        <category>DataMining</category>
      </categories>
      <tags>
        <tag>WebDataMining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224d L3 高级的词向量表示]]></title>
    <url>%2F2017%2F03%2F27%2FCS224d-L3%2F</url>
    <content type="text"><![CDATA[Advanced word vector representations: language models, softmax, single layer networks $$J(\theta)=\frac{1}{T}\sum{t=1}^{T}\sum{-c&lt;=j&lt;=c,j\neq0}logP(w_{t+j}|w_t)$$$$P(w_0|wt)=\frac{e^{v{w0}^{‘T}v{wi}}}{\sum{w=1}^{W}e^{v{w}^{‘T}v{w_i}}}$$ BGD/SGD/MBGDBGD 批量梯度下降(Batch Gradient Descent)更新每一参数时都使用所有的样本来进行更新 优点：全局最优解； 缺点：当样本数目很多时，训练过程会很慢。&gt;$\theta^{new}=\theta^{old}-\alpha\frac{\partial}{\partial\theta^{old}}J(\theta)$ SGD 随机梯度下降(Stochastic Gradient Descent)SGD是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了. 对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。 但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。 优点：训练速度快； 缺点：准确度下降，并不是全局最优。 $\theta^{new}=\theta^{old}-\alpha\frac{\partial}{\partial\theta^{old}}J_t(\theta)$ 小批量梯度下降法MBGD(Mini-batch Gradient Descent)BGD和SGD的折衷 CBOW/Skip-gram Continuous Bag-of-Words Model （CBOW） 输入单词矩阵V，有one-hot的X标明。这样$v_i=VX_i$ $v{avg}=\frac{v{c-m}+v{c-m+1}+…+v{c+m}}{2m}$求出输入层的平均值 Generate a score vector $z=Uv_{avg}$，和output和input average相乘 然后用softmax方法，$y{‘}=softmax(z)$，得到的$y{‘}$也是one hot的向量 Loss function：cross entropy交叉熵,交叉熵简单地说，就是一个事件发生的概率越大，则它所携带的信息量就越小。所以有$H(y{‘},y)=-\sum {j=1}^{|V|}y_jlog(yj^{‘})$，又因为y为one hot，所以$H(y{‘},y)$化简为$H(y_{‘},y)=-y_jlog(y_j^{‘})$。 即得出的结果符合上下文的$H ( ŷ, y ) =− 1 log ( 1 ) = 0$， 而得出的结果不符合上下文的$H ( ŷ, y ) = − 1 log ( 0.01 ) ≈ 4.605$ 而对于cost function：由softmax可得如果是对于每一个测试样本，都有$$minimize J=-logP(w{c}|w{c-m}..w{c-1}w{c+1}..w_{c+m})\=-logP(uc|v^{‘})\=-log\frac{e^{u{c}^{T}v^{‘}}}{\sum{j=1}^{|V|}e^{u{j}^{T}v^{‘}}}\=-u{c}^{T}v^{‘}+log\sum{j=1}^{|V|}e^{u_{j}^{T}v^{‘}}$$ 之后用梯度去更新相关词向量$u_c$和$v_j$ Continuous Skip-gram Model (Skip-gram)该模型是用center word预测the surrounding words。 输入中心词x为one hot向量 和CBOW一样，单词$v_c=VX$ 因为只有一个，求平均是$v_{avg}=v_c$ 利用$u=Uvc$生成2m个单词$u{c-m}，u{c-m+1}..u{c+m}$的score Turn each of the scores into probabilities, y = softmax (u) $y{c-m}，y{c-m+1}..y{c+m}$为真实y，而$y^{‘}{c-m}，y^{‘}{c-m+1}..y^{‘}{c+m}$为求出来的 Negative SamplingAppendix 第三讲：高级的词向量PPT中文版 CBOW/Skip-gram/nagative sampling见CS224d-L1/L2/L3前部分note 深度学习与自然语言处理(2)_斯坦福cs224d Lecture 2笔记中文版 softmax算法见UFLDL教程的Softmax回归 关于The skip-gram model and negative sampling，详细可见论文Distributed Representations of Words and Phrases and their Compositionality]]></content>
      <categories>
        <category>NLP/CS224d</category>
      </categories>
      <tags>
        <tag>CS224d</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224d L2 word2vec]]></title>
    <url>%2F2017%2F03%2F26%2FCS224d-L2-word2vec%2F</url>
    <content type="text"><![CDATA[1 WordNet 缺少细微差别（同义词间） 缺少新词 主观性大 人力成本 难离散化 2 “one-hot” representation$$x_{2}=\begin{bmatrix}0\0\0\1\0\\end{bmatrix}$$ 存在问题： 词汇增加，维度增大 缺少词汇间的Similarity特征 3 distributional similarity based representation3.1 full documents vs window size of（5~10） word-documents cooccurrence matrix =》 被称为LSA(latent semantic analysis 浅层语义分析 OR 隐形语义分析) inverted fille index 倒排文件索引【魏骁勇教授网络数据挖掘】 window =&gt; syntactic(POS) &amp; semantic 3.2 问题 文档增大，规模增大 维度很大(1 在很多文档出现 2 每个文档都很多词) 分类模型稀疏 不够健壮?? 3.3 解决方法：降维 SVD（奇异值分解） Learning representations by back-propagating errors(Rumelhart et al.,1986) A Neural Probabilistic Language Model(Bengio et al., 2003) Natural Language Processing (almost) from Scratch(Collobert &amp; Weston,2008) word2vec(Mikolov et al. 2013) 123456789101112131415161718192021222324252627# SVD的实现import matplotlib.pyplot as pltimport numpy as npla = np.linalgwords = [&quot;I&quot;,&quot;like&quot;,&quot;enjoy&quot;, &quot;deep&quot;,&quot;learning&quot;,&quot;NLP&quot;,&quot;flying&quot;,&quot;.&quot;]# I like deep learning.# I like NLP.# I enjoy flying.# windows size is 1# I like enjoy deep learning NLP flyingX = np.array([[0,2,1,0,0,0,0,0], # I [2,0,0,1,0,1,0,0], # like [1,0,0,0,0,0,1,0], # enjoy [0,1,0,0,1,0,0,0], # deep [0,0,0,1,0,0,0,1], # learning [0,1,0,0,0,0,0,1], # NLP [0,0,1,0,0,0,0,1], # flying [0,0,0,0,1,1,1,0]]) # .U,s,Vh = la.svd(X,full_matrices=False)print(U)plt.xlim(-0.8*1.1,0.2*1.1)plt.ylim(-0.8*1.1,0.8*1.1)for i in xrange(len(words)): # print(i) plt.text(U[i,0],U[i,1],words[i])plt.show() 3.4 the he has 等词的过滤 frequency min(t,100)：频数t与某个临界值的比较 直接ignore：频数t过大直接忽略 扩大window size：扩大size增加对临近词的计数 用皮尔逊相关系数代替计数，并置负数为0：-1&lt;相关系数r&lt;1 注：一些有趣的语义Pattern来源：An improved model of semantic similarity based on lexical co-occurence 3.5 word2vec因为SVD存在的问题： 矩阵运算量 新词的更新 不同于其他DL模型的学习框架 提出利用word2vec(Mikolov et al.2013)：目标函数 $$J(\theta)=\frac{1}{T}\sum{t=1}^{T}\sum{-c&lt;=j&lt;=c,j\neq0}logP(w_{t+j}|w_t)$$$$P(w_0|wt)=\frac{e^{v{w0}^{‘T}v{wi}}}{\sum{w=1}^{W}e^{v{w}^{‘T}v{w_i}}}$$ 求导 3.6 word2vec的线性关系apple - apples ≈ car - cars 3.7 GloVe word2vec 训练慢，统计不高效使用，但是能度量不止相似性，对其他任务也有好的表现 Based count 训练快，高效使用统计，但是主要用于相似性度量，小规模不合适 GloVe 训练更快 大规模语料算法的扩展性也很好 小语料表现得也很好 Word embedding matrix（词嵌入矩阵）,提前训练好的词嵌入矩阵 Appendix For more details, please click the note 第二讲：词向量 Softmax回归：多类别的Logistic回归 TREC(short for “Text REtrieval Conference”) 深度学习与自然语言处理(1)_斯坦福cs224d Lecture 1 笔记]]></content>
      <categories>
        <category>NLP/CS224d</category>
      </categories>
      <tags>
        <tag>CS224d</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS224d L1]]></title>
    <url>%2F2017%2F03%2F26%2FCS224d-L1%2F</url>
    <content type="text"><![CDATA[introductionNLP 语音 =》DL的突破(i.e. Speech, Computer Vision) 文本 难点 指代不明 Java hit June and then she ran. [“she” ] 模糊/二义性 He makd her duck. presentation at NLP ↓↓ Phonology 音素 ↓↓ Morphology 形态学：前缀、后缀.. ↓↓ syntax 语法 ↓↓ semantics 语义 应用 机器翻译 情感分析 QA系统 search AD speech recognition 附 The Stanford Natural Language Processing Group 有论文和demo资源 NLPIR张华平博士的汉语分词 斯坦福大学CS224d基础1：线性代数知识 笔记 CS224d2016 report CS224d Schedule and Syllabus]]></content>
      <categories>
        <category>NLP/CS224d</category>
      </categories>
      <tags>
        <tag>CS224d</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第八章 数据聚类 非监督学习方法]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB8-9%2F</url>
    <content type="text"><![CDATA[第八章 数据聚类 非监督学习方法相似性测度 欧式 马氏 明氏 相似性函数 数据标准化不是所有情况下标准化处理都是合理的。在使用标准化技术时，要注意应用的环境是否恰当。可能导致数据标准化后交叉了不易划分。 聚类的准则函数：计算完相似性后，根据准则函数来划分 误差平方和准则：一个好的聚类方法应能使集合中的所有向量与这个均值向量的误差的长度平方和最小。 散布准则：不但反映同类样本的聚集程度，而且反映不同类之间的分离程度。 子类散布矩阵 类内散布矩阵 类间散布矩阵 总散布矩阵 迹准则 行列式准则 基于模式与类核间距离的准则函数上面两种方法都是用均值向量来表示一类的位置并代替该类，损失了各类在空间中的分布情况。为了细致的表征一类，可以定义一个核来表示其模式分布结构。核可以是一个函数，一个属于同一类的模式集合或其它模型；还需要定义一个距离（即测度）以及由此构成的准则函数。 分类聚类算法 即 层次聚类算法见 网络数据挖掘笔记 聚合法： 聚合算法步骤如下，其中c是事先指定的聚类数，当c达到后，迭代停止；如果c=1，则得到整个分类树。 设c*=n，Di={xi}，i=1,2,…,n 若c*&lt;=c，则停止 找最近的两个类Di和Dj【近点距离、远点距离、平均距离】 将Di和Dj合并，删去Di， c*减1 转向步骤2 分解法：从整体开始分 动态聚类法动态聚类方法是一种普遍采用的聚类方法，主要具有以下3个要点 选定某种距离度量作为样本间的相似性度量 确定某个评价聚类结果质量的准则函数 给定某个初始分类，然后用迭代算法找出使准则函数取极值的最好聚类结果 初始聚类中心的选择方法 任取前c个样本点作为初始聚类中心 凭经验选择 将全部数据随机分为c类，计算各类重心，将重心作为聚类中心 密度法选择代表点（具有统计特性） 从c-1类划分中产生c类划分问题的初始聚类中心 初始聚类中心确定后，有不同的分类方法来确定初始划分，包括如何修正聚类中心 对选定的中心按距离最近原则将样本划归到各聚类中心代表的类别，然后调整聚类中心（批量修正法） 取一样本，将其归入与其距离最近的那一类，并计算该类的样本均值，以此样本均值代替原来的聚类中心作为新的聚类中心，然后再取下一个样本，如此操作，直到所有样本都归属到相应的类别中为止（单步样本修正法） 一般来说，不同的初始划分往往会得到不同的解。 K均值算法 给定允许误差ℇ，令t=1 初始化聚类中心wi(t)，i=1,2,…,c 修正dij， 修正聚类中心wi(t+1) 计算误差E或者Je 如果E&lt; ℇ ，则算法结束；否则t=t+1，转步骤3 上述K均值算法每次把全部样本都调整完毕后才重新计算一次各类的聚类中心，属于批处理算法；也可以采用逐个样本修正法，每调整一个样本的类别就重新计算一次各类的聚类中心。 这个算法是在类别数c给定的情况下进行的。当类别数未知时，可以假设类别是在不断增加的，准则函数是随c的增加而单调减小的。可以通过Je-c的关系曲线来确定合适的聚类类别数。 ISODATA算法 合并发生在某一类样本个数太少，或者两类聚类中心之间距离太小的情况 分裂发生在某一类别的某分量出现类内方差过大的现象 设置若干控制参数 K均值算法的迭代次数 控制合并与分裂的参数 最多合并次数 聚类中最少样本数 控制分裂参数 最小类间距离 合并与分裂次数 算法步骤 选择参数 确定初始聚类中心 用K均值算法进行迭代。 合并/分裂 计算各类的新的聚类中心 判断是否满足结束条件，否则转3 第九章 模糊模式识别]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第七章 特征提取和选择]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB7%2F</url>
    <content type="text"><![CDATA[第七章 特征提取和选择特征形成特征通常有物理的、结构的、数学的三种类型。物理的、结构的比较直观，但对于计算机而言，数学的更方便处理。因此，对于一个具体问题，往往是， 物理量的获取与转换：对从传感器中得到的信号，可以称之为原始信息 描述事物方法的选择与设计：在得到了原始信息之后，要对它进一步加工，以获取对分类最有效的信息。 特征空间的优化 ：已有了一个初始的特征空间后，如何对它进行改造与优化的问题。一般说来要对初始的特征空间进行优化是为了降维。 特征形成：根据被识别的对象产生出一组基本特征，它可以是由计算得到的，也可以是用仪表或传感器测量出来的，这样产生出来的特征称为原始特征。在大多数情况下，不能直接对原始特征进行分类器设计。 特征选择：从一组特征中挑选出一些最有效的特征以达到降低特征空间维数的目的，这个过程叫特征选择。 特征抽取：原始特征的数量可能很大，或者样本处于一个高维空间中，通过映射（或变换）的方法可以用低维空间来表示样本，这个过程叫特征抽取。映射后的特征叫二次特征，它们是原始特征的某种组合（通常是线性组合）。所谓特征抽取在广义上就是指一种变换。注：特征抽取是通过变换的方法组合原始高维特征，获得一组低维的新特征，而特征选择是根据专家的经验知识或根据某种评价准则来挑选出那些对分类最有影响力的特征，并未形成新的特征。例子：需要处理： 比如车牌识别，数据扫描进来后，怎么来表示数字呢？ 可以直接图像， 可以是数字自上到下离y轴的距离，10个数字这类信息肯定是不一样且小于上述图像表示的 可以是田字格中，数字和横向线、竖向线的交点数量。 类别可分离性判据对特征空间优化是一种计算过程： * 找到一种准则（或称判据），通常用一种式子表示 * 计算出一种优化方法，使得这种计算准则达到一个极值。 基于距离的可分性判据：使类间距离尽可能大同时又保持类内距离较小类的均值向量：点到点集的距离：注：此处以均值向量为衡量，在K-means算法中提过也可以用最近、最远等为判断其他基于距离的可分离性判据： 类内离散度矩阵 两类之间的距离 各类模式之间的总的均方距离 多类情况下总的类内、类间及总体离差矩阵 基于概率分布的可分性判据满足下述条件的任何函数都可以用作基于类概率密度的可分性判据。 非负性 当两类概率密度函数完全不重叠时，判据取最大值 当两类概率密度函数完全相同时，判据取最小值0 特征抽取:低维新特征原始特征的数量可能很大，或者样本处于一个高维空间中，通过映射（或变换）的方法可以用低维空间来表示样本，这个过程叫特征抽取。映射后的特征叫二次特征，它们是原始特征的某种组合（通常是线性组合）。所谓特征抽取在广义上就是指一种变换。 基于可分离性判据的特征抽取方法 基于离散K-L变换的特征抽取方法 即 主成分分析K-L变换是一种正交变换，即将一个向量X，在某一种坐标系统中的描述，转换成用另一种基向量组成的坐标系统表示。要使得转换坐标系后的向量和原向量的均方误差为最小：$$\zeta = E[(X-\hat X)^T(X-\hat X)]$$优点： 可以使变换后所生成的新分量正交或不相关 K-L变换后的协方差矩阵为对角矩阵 在用较少的新分量来表示原特征向量时，可以达到均方误差最小 例子： 降维与数据压缩是紧密联系在一起的。如原训练样本集的数量为V，而现采用30个基(每个基是一幅图象)，再加上每幅图象的描述参数，数据量是大大降低。尤其是图象数很大时，压缩量是十分明显的。利用K-L变换进行人脸图象识别是一个著名的方法。 其原理十分简单，首先搜集要识别的人的人脸图象，建立人脸图象库， 然后利用K-L变换确定相应的人脸基图象， 再反过来用这些基图象对人脸图象库中的有人脸图象进行K-L变换，从而得到每幅图象的参数向量并将每幅图的参数向量存起来。 在识别时，先对一张所输入的脸图象进行必要的规范化，再进行K-L变换分析，得到其参数向量。 将这个参数向量与库中每幅图的参数向量进行比较，找到最相似的参数向量，也就等于找到最相似的人脸，从而认为所输入的人脸图象就是库内该人的一张人脸, 完成了识别过程 注：一个非周期性随机过程不能用具有互不相关的随机傅立叶系数的傅立叶级数表示，但是可以用具有互不相关系数的正交函数的级数展开。K-L展开式就是这样一种展开方法。 1 离散有限K-L展开 2 基于K-L变换的数据压缩问题：如果从n个特征向量中选取m个组成新的变换矩阵，m&lt;n，那么怎么选取m个特征向量可以在最小均方误差准则下效果最优？方法：选择m个最大的特征值对应的特征向量组成新的变换矩阵，可以让最小均方误差最小。因此，K-L变换又称为主成分分析。算法： 平移坐标系，将模式的总体均值向量作为新坐标系的原点 求随机向量X的自相关矩阵$R=E(XX^T)$ 求自相关矩阵的n个特征值及其对应的特征向量 将特征值从大到小排序，取前m个大的特征值所对应的特征向量构成新的变换矩阵 将n维向量变换为m维新向量 例子： 特征选择：专家选择，删除部分特征从一组特征中挑选出一些最有效的特征以达到降低特征空间维数的目的，这个过程叫特征选择。因此有两个关键问题：选择的标准、搜索的算法 标准：搜索： 最优搜索算法最优搜索算法：至今能得到最优解的唯一快速算法是“分支定界”算法。属于自上而下的算法，具有回溯功能。算法核心是通过合理组合搜索过程，避免一些重复计算。关键是利用了判据的单调性。如果$æ_i&lt;æ_j$，则有$J(æ_i)≤J(æ_j)$ 次优搜索算法顺序前进法（SFS）最简单的自下而上法，每次选取一个，使得与已经选入的特征组合判据最大 一般说来，由于考虑了特征之间的相关性，在选择特征时计算与比较了组合特征的判据值，要比前者好些。其主要缺点是，一旦某一特征被选入，即使由于后加入的特征使它变为多余，也无法再把它剔除 该法可推广至每次入选r个特征，而不是一个，称为广义顺序前进法(GSFS)。顺序后退法（SBS）最简单的自上而下法 这是一种与SFS相反的方法，是自上而下的方法。做法也很简单，从现有的特征组中每次减去一个不同的特征并计算其判据，找出这些判据值中之最大值，如此重复下去直到特征数达到予定数值d为止与SFS相比，此法计算判据值是在高维特征空间进行的，因此计算量比较大。 此法也可推广至每次剔除r个，称为广义顺序后退法（GSBS） 增l减r法在选择过程中加入局部的回溯 前面两种方法都有一个缺点，即一旦特征入选(或剔除)，过程不可逆转。为了克服这种缺点，可采用将这两种方法结合起来的方法，即增l减r法。 其原理是对特征组在增加l个特征后，转入一个局部回溯过程，又用顺序后退法，剔除掉r个特征。这种方法既可能是“自上而下”方法，也可能是“自下而上”的，这取决于l与r的数据大小。当l＜r时，入选特征数逐渐增加，属“自下而上”型，反之属“自上而下”型。增 l减r法的推广增 l减r法也可推广至用GSFS及GSBS代替SFS及SBS，并可在实现增加l特征时采用分几步实现。增l特征用$Z_l$步，减r则用$Z_r$步，该种方法一般称为$（Z_l, Z_r）$法。这种做法是为了既考虑入选(或剔除)特征之间的相关性，又不至因此引起计算量过大。合理地设置$Z_l$与$Z_r$ ，可以同时对两者，即计算复杂性及特征选择的合理性兼顾考虑。 $（Z_l, Z_r）$算法前面所讲的各种方法都可看作是$（Z_l, Z_r）$法的特例，它们的关系如下所示：$Z_l=(1), Z_r=(0)$：SFS（1, 0）算法$Z_l=(0), Z_r=(1)$：SBS（0, 1）算法$Z_l=(d), Z_r=(0)$：穷举法$Z_l=(l), Z_r=(0)$： GSPS(l)算法$Z_l=(0), Z_r=(r)$： GSBS(r)算法$Z_l=(1,1,…,1), Z_r=(1,1,…,1)：(l, r)$算法 新的优化搜索算法 遗传算法：建立在生物进化基础上的，是一种基于自然选择和群体遗传机理的搜索算法。主要有编码、选择、交换、变异等操作。参考博客 模拟退火算法：从某一较高初温出发，伴随温度参数的不断下降，结合概率突跳特性在解空间中随机寻找目标函数的全局最优解，即在局部最优解能概率性地跳出并最终趋于全局最优。 Tabu搜索算法：隐含假定一个解的“邻域”中往往存在性能更好的解，构建Tabu表记录近期搜索过的性能较优的解。 蚁群算法 ant colony optimization, ACO 参考博客 A*]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第六章 近邻法则]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB6%2F</url>
    <content type="text"><![CDATA[第六章 其他分类方法 近邻法则最近邻法算法：新样本属于 最近的训练样本所属的类别 缺点：偶然性真大 K近邻法最近邻法的推广。k Nearest Neighbor Classification Rule算法：最近的k个训练样本，所属的类别最多的类，即新样本的类。因此k一般选奇数 模拟k近邻法k近邻法当样本不均衡时，远离测试样本的样本点会产生很大干扰。算法：可以采用模糊分类的思想，引入隶属度函数的概念，对K个近邻的样本点的贡献加权，来进行分类判决。 改进的近邻法 快速搜索近邻法 剪辑近邻法利用现有样本集对其自身进行剪辑，将不同类别交界处的样本以适当方式筛选，可以实现既减少样本数又提高正确识别率的双重目的。 两分剪辑近邻法算法：把原样本集分为样本集和测试集，用测试机的数据来筛选样本集。 重复剪辑近邻法算法： 压缩近邻法剪辑近邻的结果只是去掉了两类边界附近的样本，而靠近两类中心的样本几乎没有被去掉。在剪辑的基础上，再去掉一部分这样的样本，有助于进一步缩短计算时间和降低存储要求。这类方法叫作压缩近邻法。 算法： 压缩近邻法中定义了两个存储器，一个用来存放即将生成的样本集，Store；另一个存放原样本集，Grabbag。 初始化。随机挑选一个样本放在Store中，其它样本放入Grabbag。 用当前的Store中的样本按最近邻法对Grabbag中的样本分类。假如分类正确，该样本放回Grabbag；否则放入Store。 重复上述过程，直到在执行中没有一个样本从Grabbag转到Store或者Grabbag为空。 优缺点 近邻法是典型的非参数法，其优点是 实现简单 分类结果比较好，在训练样本N→∞时接近最优 近邻法的主要缺点是 对计算机的存储量和计算量的要求很大，耗费大量测试时间 没有考虑决策的风险。 对其错误率的分析都是建立在渐进理论基础上的。]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第四 五章 线性分类器 非线性判别函数]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB4-5%2F</url>
    <content type="text"><![CDATA[第四章 线性分类器线性分类器、广义线性分类器贝叶斯分类器：最优分类器线性判别函数产生的错误率或风险可能更大，但实现容易。准则函数包括：感知准则、最小平方误差准则、最小错分样本数准则、Fisher准则 类内类间等要记住线性判别函数：c类问题=c个的两类问题或者 c*（c-1）/2个线性判别式 Fisher线性判别准则类间大、类内小： 感知准则函数例子：用梯度下降法： 最小平方误差准则感知准则函数及其梯度下降法只适用于样本线性可分的情况，对于线性不可分情况，迭代过程永远不会终结，即算法不收敛。在实际问题中常常无法事先知道样本集是否线性可分，因此希望找到一种既适用于线性可分又适用于线性不可分的算法。通过这种算法得到的解都统一称为最优解。例子： 最小错分样本数准则对于不等式$w^Tx^i &gt;0$，如果有解，可以得到解向量$$w^$$，如果无解，则说明样本集线性不可分，那么对于任何向量w，必然有某些样本被错分，此时我们可以寻找使最多数目的不等式得到满足的权向量，将它作为最优解向量$$w^$$。上述准则便是最小错分样本数准则的基本思想。 第五章 非线性判别函数由于样本在特征空间分布的复杂性，很多情况下采用线性判别函数不能取得满意的效果。采用分段线性判别或二次函数判别等非线性方法效果会好得多。 分段线性判别函数 二次判别函数]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第三章 概率密度函数的估计]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB3%2F</url>
    <content type="text"><![CDATA[第三章 概率密度函数的估计比贝叶斯决策多了估算先验概率和类条件概率密度函数在实际中先验概率和类条件概率密度函数常常是未知的。设计分类器的过程一般分为两步，称为基于样本的两步贝叶斯决策。 利用样本集估计类条件概率密度与先验概率 将估计量代入贝叶斯决策规则，完成分类器设计 在统计学上，有以下几个常用标准和概念： 无偏性：参数x的估计量$(x_1,x_2..x_n)$的数学期望等于x，则称估计是无偏的，当样本数趋于无穷时才有的无偏性称为渐进无偏 有效性：方差更小的估计，更有效 一致性：如果有 $\lim\limits_{n\to\infty}P(|\hat x - x|&gt;\epsilon)=0$，则称为一致估计 点估计、区间估计。统计量、参数空间、基于平均和方差的评价 整体思路： 如何利用样本集估计$\hat p(x|w_i)$和$\hat p(w_i)$ 在典型的有监督模式识别问题中，估计先验概率较为简单。 类条件概率密度估计难度在于：一般来说，训练样本总数较少；x的维数较大时，存在计算复杂度等问题。 从样本集推断总体概率分布的方法可以归纳为2种 参数估计：总体概率密度函数形式已知，但某些参数未知。 监督参数估计：样本所属类别已知 非监督参数估计：未知样本类别 非参数估计：已知样本类别，未知总体概率密度函数形式，要求直接推断概率密度函数本身。 估计量的性质如何 利用样本集估计错误率的方法 最大似然估计(一般用这个)待估计参数被看做是确定性的，取值未知的量。最佳估计就是使得产生已观测到的样本的概率为最大的那个值。 例子： 贝叶斯估计把待估计参数看做是符合某种先验概率分布的随机变量。对样本进行观测的过程，就是把先验概率密度转换为后验概率密度，使用样本信息修正对参数的估计值。贝叶斯决策与贝叶斯估计两者都是立足于使贝叶斯风险最小，只是要解决的问题不同，前者是要决策x的真实状态，而后者则是要估计X所属总体分布的参数。 对比总结 贝叶斯估计方法有很强的理论和算法基础，但在实际应用中，最大似然估计更为简便。 用本章介绍的估计结果以及第二章贝叶斯决策来设计分类器时，最终系统误差来源可能如下 贝叶斯误差：由p(x|ωi)之间的互相重叠造成，不可消除。 模型误差：选择了不正确的模型导致。可消除设计者根据对问题的先验知识和理解来选择模型 估计误差：采用有限样本进行估计造成的误差。可以通过增加样本个数来减小。]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模式识别 第二章 统计决策方法 贝叶斯决策理论]]></title>
    <url>%2F2017%2F03%2F26%2F%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB2%2F</url>
    <content type="text"><![CDATA[卢晓春 模式识别引论第二章 统计决策方法 贝叶斯决策理论 先验概率=已知的类别的概率，如二分类$P(w_1)+P(w_2)=1$类条件概率密度函数：$P(x|w_1),P(x|w_2)…P(x|w_n)$后验概率：分类判决的依据$P(w_1|x)+P(w_2|x)=1$比如某地区的癌细胞概率，病人 不同阈值标准： 最小错误率贝叶斯决策$$P(w_i|x)=\frac{P(x|w_i)P(w_i)}{P(x)}=\frac{P(x|w_i)P(wi)}{\sum{j=i}^{2}P(x|w_j)P(w_j)},i=1,2$$即求后验概率更大的情况i=$argumax(P(w_i|x))$，又$P(w_i|x)$中分母相同，即求i=$argumax(P(x|w_i)P(w_i))$整理成似然比形式：$$\begin{split} P(x|w_1)P(w_1)&amp;&gt;P(x|w_2)P(w_2) \l(x)=\frac{P(x|w_1)}{P(x|w_2)}&amp;&gt;\frac{P(w_2)}{P(w_1)}\begin{cases}true, &amp;\quad x \in w_1 \false,&amp;\quad x \in w_2\end{cases}\end{split} $$例题： 某地区正常细胞和异常细胞的先验概率为$P(w_1)=0.9,P(w_2)=0.1$某人的细胞观察值x，根据类条件概率密度函数,$P(x|w_1)=0.2,P(x|w_2)=0.4$因为$\frac{P(x|w_1)}{P(x|w_2)}&gt;\frac{P(w_2)}{P(w_1)}$，所以属于第一类，判定是正常细胞。注：图中交叉部分，实际为绿可是判定为红，或则反之，都是错误的判定 最小风险贝叶斯决策决策表 决策 类别 类别 风险值 $\omega_1$ $\omega_2$ $$a_1$$ $$\lambda_{11}$$ $$\lambda_{12}$$ $$a_2$$ $$\lambda_{21}$$ $$\lambda_{22}$$ 将后验概率$P(w_i|x)$乘上风险$$R(a1|x)=\sum{j=1}^2\lambda_{1j}P(w_j|x) $$$$R(a2|x)=\sum{j=1}^2\lambda_{2j}P(w_j|x) $$比较，R较小的那个为最终决策。 例题 决策 类别 类别 风险值 $\omega_1$ $\omega_2$ $a_1$ 0 6 $a_2$ 1 0 某地区正常细胞和异常细胞的先验概率为$$P(w_1)=0.9,P(w_2)=0.1$$某人的细胞观察值x，根据类条件概率密度函数,$$P(x|w_1)=0.2,P(x|w_2)=0.4$$因为$$R(a1|x)=\sum{j=1}^2 \lambda_{1j}P(wj|x)=0+\lambda{12}P(w_2|x)=1.092$$，$$R(a2|x)=\sum{j=1}^2 \lambda_{1j}P(wj|x)=\lambda{21}P(w_1|x)+0=0.818$$所以为了风险最小，判定为第二类 Neyman-Person决策 奈曼皮尔逊：限定一类错误率为常数而使另一类错误率最小的决策规则。 第一类错误率$$P1(e)=\int{R_2}P(x|w_1)dx$$ 第二类错误率$$P2(e)=\int{R_1}P(x|w_2)dx$$ 整个特征空间$R=R_1+R_2$【Lagrange乘子法】：$$mini \gamma = P_1(e)+\lambda(P_2(e)-\epsilon_0)$$又错误=1-正确：$$P1(e)=\int{R_2}P(x|w1)dx=1-\int{R_1}P(x|w1)dx$$代入：$$\begin{split}\gamma &amp;= 1-\int{R_1}P(x|w_1)dx+\lambda P_2(e)-\lambda \epsilon_0 \&amp;=(1-\lambda \epsilon0)+\lambda \int{R_1}P(x|w2)dx-\int{R_1}P(x|w_1)dx \&amp;=(1-\lambda \epsilon0)+ \int{R_1}[\lambda P(x|w_2)-P(x|w_1)]dx \\end{split}$$分别对$\lambda$和分界面t求导，得到$$\lambda=\frac{P(x|w_1)}{P(x|w2)}$$且$$\int{R_1}P(x|w_2)dx=\epsilon_0$$而在分界面处，可以看到$$[\lambda P(x|w_2)-P(x|w_1)]\lambda ,\begin{cases}true, &amp;\quad x \in w_1 \false,&amp;\quad x \in w_2\end{cases}$$ 最小最大决策：保守的，在最差情况下有好的结果。 另有ROC曲线见吴恩达《机器学习》课程]]></content>
      <categories>
        <category>ML/卢晓春 模式识别引论</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week11 OCR]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week11-OCR%2F</url>
    <content type="text"><![CDATA[第十一周18 应用实例：图片文字识别(Application Example: Photo OCR)18.1 问题描述和流程图18.2 滑动窗口18.3 获取大量数据和人工数据18.4 上限分析：哪部分管道的接下去做19 总结(Conclusion)19.1 总结和致谢]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week10 Large Scale]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week10-Large-Scale%2F</url>
    <content type="text"><![CDATA[第十周17 大规模机器学习(Large Scale Machine Learning)17.1 大型数据集的学习17.2 随机梯度下降法17.3 小批量梯度下降17.4 随机梯度下降收敛17.5 在线学习17.6 映射化简和数据并行]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week9 异常检查&推荐系统]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week9-%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[第九周15 异常检测(Anomaly Detection)15.1 问题的动机15.2 高斯分布15.3 算法15.4 开发和评价一个异常检测系统15.5 异常检测与监督学习对比15.6 选择特征15.7 多元高斯分布（可选）15.8 使用多元高斯分布进行异常检测（可选）16 推荐系统(Recommender Systems)16.1 问题形式化16.2 基于内容的推荐系统16.3 协同过滤16.4 协同过滤算法16.5 向量化：低秩矩阵分解16.6 推行工作上的细节：均值归一化]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week8 Unsupervised Learning]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week8-Unsupervised-Learning%2F</url>
    <content type="text"><![CDATA[第八周 Unsupervised Learning13 聚类(Clustering)13.1 无监督学习：简介13.2 K-均值算法13.3 优化目标13.4 随机初始化13.5 选择聚类数14 降维(Dimensionality Reduction)14.1 动机一：数据压缩14.2 动机二：数据可视化14.3 主成分分析问题 PCA14.4 主成分分析算法14.5 选择主成分的数量14.6 重建的压缩表示14.7 主成分分析法的应用建议]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week7 SVM]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week7-SVM%2F</url>
    <content type="text"><![CDATA[第七周 SVM12 支持向量机(Support Vector Machines)在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式：SVMJuly：支持向量机通俗导论（理解SVM的三层境界） 12.1 优化目标12.2 大边界的直观理解12.3 数学背后的大边界分类（可选）12.4 核函数 112.5 核函数 212.6 使用支持向量机]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week6 评估&设计]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week6-%E8%AF%84%E4%BC%B0-%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[第六周10 Advice for Applying Machine Learning10.1 决定下一步做什么当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？: 获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。 尝试减少特征的数量 尝试获得更多的特征 尝试增加多项式特征 尝试减少归一化程度 λ 尝试增加归一化程度 λ 10.2 评估一个假设把样本数据分为训练集、测试集，以此来评估模型。 10.3 模型选择和交叉验证集假设我们要在 10 个不同次数的二项式模型之间进行选择，我们需要使用 60%的数据作为训练集，使用20%的数据作为交叉验证集， 使用 20%的数据作为测试集。模型选择的方法为： 使用训练集训练出 10 个模型 用 10 个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值） 选取代价函数值最小的模型 用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值） 10.4 诊断偏差和方差 对于训练集，当 d 较小时，模型拟合程度更低，误差较大；随着 d 的增长，拟合程度提高，误差减小。 对于交叉验证集，当 d 较小时，模型拟合程度低，误差较大；但是随着 d 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。 训练集误差和交叉验证集误差近似时： 偏差/欠拟合 交叉验证集误差远大于训练集误差时： 方差/过拟合 10.5 归一化和偏差/方差选择 λ 的方法为： 使用训练集训练出 12 个不同程度归一化的模型 用 12 模型分别对交叉验证集计算的出交叉验证误差 选择得出交叉验证误差最小的模型 运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集交叉验证集模型的代价函数误差与λ的值绘制在一张图表上： 当 λ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大 随着 λ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加 10.6 学习曲线学习曲线：训练集误差和交叉验证集误差作为训练集实例数量（ m）的函数绘制的图表应用： 在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助 利用学习曲线识别高方差/过拟合：当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。 10.7 决定下一步做什么 获得更多的训练实例——解决高方差 尝试减少特征的数量——解决高方差 尝试获得更多的特征——解决高偏差 尝试增加多项式特征——解决高偏差 尝试减少归一化程度 λ——解决高偏差 尝试增加归一化程度 λ——解决高方差 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小 使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过归一化手段来调整而更加适应数据。 通常选择较大的神经网络并采用归一化处理会比采用较小的神经网络效果要好。 对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络 11 机器学习系统的设计(Machine Learning System Design)11.1 首先要做什么列出可能的方向：当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。在11.2会教导如何更可能地选择一个真正的好方法，然后花精力去研究。 11.2 误差分析error analysis选出正确的方向：构建一个学习算法的推荐方法为： 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法 绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择 进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势（然后从出现次数最多的情况开始着手优化） 误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。 11.3 类偏斜的误差度量Error Metrics for Skewed Classes类偏斜情况：表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。因为样本的偏差，所有误差分析此时不能作为评判算法的标准，因此提出另一方法来作为偏斜类问题的评估度量值查准率（Precision）和查全率（Recall）我们将算法预测的结果分成四种情况： 正确肯定（ True Positive,TP）：预测为真，实际为真 正确否定（ True Negative,TN）：预测为假，实际为假 错误肯定（ False Positive,FP）：预测为真，实际为假 错误否定（ False Negative,FN）：预测为假，实际为真则：查准率=TP/（ TP+FP）例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。查全率=TP/（ TP+FN）例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。 11.4 查全率和查准率之间的权衡 目的 方法 结果 更高的查准率 使用比 0.5 更大的阀值，如 0.7， 0.9 减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。 提高查全率 使用比 0.5 更小的阀值，如 0.3 尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以。 查全率与查准率的关系： $F_{1}Score=\frac{PR}{P+R}$ 11.5 机器学习的数据在机器学习中的普遍共识： “取得成功的人不是拥有最好算法的人， 而是拥有最多数据的人”。关键的假设：|条件|结果||:-:|:-:||特征值有足够的信息量， 且有一类很好的函数|这能保证低误差||有大量的训练数据集|这能保证得到更多的方差值|]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week5 BP]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week5-BP%2F</url>
    <content type="text"><![CDATA[第五周 BP9 Neural Networks: Learning9.1 代价函数2017年2月22日22:28:15 9.2 反向传播算法9.3 反向传播算法的直观理解9.4 实现注意：展开参数9.5 梯度检验9.6 随机初始化9.7 综合起来9.8 自主驾驶]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week4 Neural Networks]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week4-Neural-Networks%2F</url>
    <content type="text"><![CDATA[第四周8 Neural Networks: Representation8.1 非线性假设假使我们采用的都是 50x50 像素的小图片，并且我们将所有的像素视为特征，则会有2500 个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约 25002/2 个（接近 3 百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要神经网络。 8.2 神经元和大脑略 8.3 模型表示 1 8.4 模型表示 2 8.5 特征和直观理解 1在普通的逻辑回归中，我们被限制为使用数据中的原始特征x1,x2,…,xn，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。 8.6 样本和直观理解 II略 8.7 多类分类多类分类只需要输出层的label向量维度更大即可。]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week3 罗杰斯特回归&正则化]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week3-%E7%BD%97%E6%9D%B0%E6%96%AF%E7%89%B9%E5%9B%9E%E5%BD%92-%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[第三周6 Logistic Regression 逻辑回归/两分类问题6.1 分类问题Classification and Representation分类问题：0或1坏样本很大影响预测linear function 6.2 假说表示 Hypothesis Representation$h_\theta(x)=g(\theta^Tx)$逻辑函数、激励函数$g(z)=\frac{1}{1+e^{-z}}$ 6.3 判定边界 Decision Boundary：output0,1的input间的边界 linear decision boundary non-linear decision boundary eg.$h_\theta(x)=g(\theta^T(x))&gt;=0.5$when$\theta^T(x)&gt;=0$，$\theta^T(x)$就是边界 6.4 代价函数Logistic Regression Model cost function:convex$Cost(h_\theta(x_i) ，y_i)$ 6.5 简化的成本函数和梯度下降 simplified cost function and gradient descent$$Cost(h_\theta(x_i) ，yi)=-ylog(h\theta(x))-(1-y)log(1-h_\theta(x))$$ We can fully write out our entire cost function as follows: $$h=g(\theta^T X)$$$$J(\theta)=\frac{-1}{m}\sum_{i=1}^{m}[yi log(h\theta(x_i))+(1-yi)log(1-h\theta(x_i))]$$ Gradient DescentRemember that the general form of gradient descent is:$$\theta_j:=\theta_j-\alpha \frac{\sigma J(\theta)}{\sigma \theta_j}$$Repeat:$$\theta_j:=\thetaj-\frac{\alpha}{m}\sum{i=1}^{m}(h_\theta(x_i)-yi)x{ij}$$ Partial derivative of J(θ)First calculate derivative of sigmoid function:$σ(x)′=(\frac{1}{1+e^{−x}})′=σ(x)(1−σ(x))$Now we are ready to find out resulting partial derivative:$$\frac{\sigma J(\theta)}{\sigma \thetaj}=\frac{1}{m}\sum{i=1}^{m}(h_\theta(x_i)-yi)x{ij}$$ 6.6 高级优化 advanced optimization高级优化算法： 共轭梯度法 BFGS (变尺度法) L-BFGS (限制变尺度法 这三种算法收敛得远远快于梯度下降。同时这三种算法内部有线性搜索(line search)算法，来尝试不同的学习率$\alpha$。 6.7 多类别分类：一对多 Multiclass Classification 7 正则化(Regularization) 如上所示，x次数越高拟合越好，但是预测能力会减弱。 7.1 过拟合的问题解决过拟合问题： 丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征， 或者使用一些模型选择的算法来帮忙（例如 PCA） 正则化。 保留所有的特征，但是减少参数的大小（ magnitude） 正则化的目的：防止过拟合！ 正则化的本质：约束（限制）要优化的参数。 7.2 代价函数7.1中$$h_\theta(x)=θ_0+θ_1x_1+θ_2x_2^2…$$式子可以看出，是那些高次项导致了过拟合的产生，如果我们能让这些高次项的系数接近于 0 的话，我们就能很好的拟合了。因此我们选择加大最小化代价函数过程中对高次项的惩罚。如下式对所有θ都增大了惩罚： $$J(θ)= \frac{1}{2m}[\sum_{1}^{m}(h(x_i) - yi)^2+\lambda\sum{j=1}^n\theta_i^2]$$此处$$\lambda$$过大，则会把所有的参数都最小化了，导致模型变成 hθ(x)=θ0,过小，起不到作用。 7.3 正则化的线性回归 7.4 正则化的逻辑回归]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week2 Linear Regression with Multiple Variable]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week2-Linear-Regression-with-Multiple-Variable%2F</url>
    <content type="text"><![CDATA[第二周4 多变量线性回归(Linear Regression with Multiple Variables)4.1 多维特征Multiple Features例如房子的多维特征$$x{2}=\begin{bmatrix}1\3\\end{bmatrix}$$支持多变量的假设$$h\theta(x)=θ_0+θ_1x_1+…θ_nx_n$$ 即：$h_\theta(x)=θ^{T}X$ 4.2 多变量梯度下降代价函数：$$J(θ_0,θ_1..θn)= \frac{1}{2m}\sum{1}^{m}(h(x_i) - y_i)^2$$同样，根据批量梯度下降算法：$$\theta_n=\thetan-\alpha\frac{1}{m}\sum^{m}{i=1}(h_\theta(x_i)-yi)x{n,i}$$ 4.3 梯度下降法实践 1-特征缩放使得所有特征的尺度都尽量缩放到-1到1之间 4.4 梯度下降法实践 2-学习率学习率即步长：通常有 α=0.01， 0.03， 0.1， 0.3， 1， 3， 10 4.5 特征和多项式回归线性回归不一定符合数据本身。因此有时需要曲线来适应数据。如：$h_\theta(x)=θ_0+θ_1x_1+θ_2x_2^2$注：如果我们采用多项式回归模型，在运行梯度下降算法前， 特征缩放非常有必要 4.6 正规方程梯度下降算法来寻找cost的最小值很方便，但是对于某些线性回归问题，正规方程方法是更好的解决方案。 (如抛物线的最小值) 4.7 正规方程及不可逆性5 安装matlab、Octave，Matlab入门]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CS229 Week1 Linear Regression with-One Variable]]></title>
    <url>%2F2017%2F03%2F26%2FCS229-Week1-Linear-Regression-with-One-Variable%2F</url>
    <content type="text"><![CDATA[吴恩达的Machine LearningMachine Learning是Coursera上的一个课程。很受入门者的推崇。我在学习了BP算法(Back Propagation反向传播)之后开始观看。 第一周1 Introduction1.1 welcome机器学习很有用也很有市场。 1.2 what is ML 学习算法主要是监督学习和无监督学习 如何选择学习算法 1.3 监督学习Supervised Learning两个例子： Regression回归问题（房价）：连续值 Classification分类问题（癌症）：离散值 1.4 无监督学习Unsupervised Learning 聚类算法作用：不知道数据集的信息，机器来找到不同的聚类和结构关系例子： google news(相关新闻会聚集在一起) 很多人的基因放在一起，来自动的辨认个体类型 应用于机器的协同工作 应用于社交网络分析 应用于公司客户的细分市场，针对性营销 应用于天文科学 应用于嘈杂声音中的区分（鸡尾酒会算法） 2 单变量线性回归(Linear Regression with One Variable)2.1 - Model Representation 模型表示我们的第一个学习算法是线性回归算法。例子：房价预测通过单变量size of house和price的关系，求拟合函数，用于预测价格。 training set learning algorithm size of house -&gt;hypothesis-&gt; estimated price $h(x)=θ_0+θ_1*x$ 2.2 代价函数在建模的过程中，引入的parameters $ \theta_0$和$\theta_1$的初始值会导致建模误差(modeling error)，为了预测房价的准确，我们应该使得误差尽量小，于是引入cost function来衡量误差。包括回归问题在内的大多数问题，cost function用误差平方代价函数都是很好用的： cost function：$$(θ_0,θ1)= \frac{1}{2m} \sum{i=1}^{m} (h(x_i)-y_i)^2$$ 2.3 代价函数的直观理解 hypothesis：$h(x)=θ_0+θ_1*x$ Parameters:$\theta_0$和$\theta_1$ Cost function:$$J(θ_0,θ1)= \frac{1}{2m}\sum{1}^{m}(h(x_i) - y_i)^2$$ Goal:$minJ(\theta_0,\theta_1)$简化假设：$θ_0=0$即$h(x)=θ_1*x$，如此$J(θ_1)$变成一个二次函数，有最低点 2.4 代价函数的直观理解II 等线图（contour plot）：若不做简化，则价值函数就从平面二次函数变为三维中的曲面（MATLAB实现），就可以在自变量平面上画等值线 2.5 梯度下降 Gradient Descent有了直观感受，我们的目标降低$J(\theta)$就是找到最低点。注：正规方程(normal equations)也能求J函数的最小值，但是计算量大，因此用梯度下降法 梯度下降：设置$(\theta_0,\theta_1,…\theta_n)$的初始值，按照梯度下降(下降最多的方向)设置新的$\theta$ 批量梯度下降法的公式： 批量梯度下降法Batch gradient descent:每一步都用到了所有数据$\alpha$：learning rate$\theta$：同步更新 2.6 梯度下降的直观理解梯度下降中，计算的$\frac{\sigma J(\theta)}{\sigma \theta_j}$可以说是斜率，当该值很小的时候，可能就是到达最小值了，但也可能是局部最小值。 收敛到一个局部最小解上。或者凸函数（convex function）是全局最小值 2.7 梯度下降的线性回归 Gradient Descent For Linear Regression回到之前的房价问题： 我们将梯度下降法运用到线性回归模型上，得到梯度下降的公式： 此处为”批量梯度下降”，在梯度下降每一步中，在计算微分求导项时，我们需要进行求和运算 3 Linear Algebra Review 线性代数略略略]]></content>
      <categories>
        <category>ML/CS229</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F03%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Firstthanks the detailed blogs of fzy-Line secondthere are some problems when using MathJax in the Hexo environment. It’s a mistake of regularization. According to this passage, I unstill the hexo-renderer-kramed and install the engine hexo-renderer-marked. Quick Start12345$ hexo new "My New Post"$ hexo server$ hexo clean$ hexo generate$ hexo deploy]]></content>
      <categories>
        <category>Diary</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning is Fun]]></title>
    <url>%2F2016%2F09%2F10%2FMachine-Learning-is-Fun%2F</url>
    <content type="text"><![CDATA[Machine Learning is Fun正值课程需要，想起起初有试着阅读Martin T.Hagan等人编写的《神经网络设计》和周志华的《机器学习》。但是现在阅读了一篇Adam Geitgey教授的《Machine Learning is Fun!》，“举个例子”让我颇为兴趣。感谢搬运工的中文翻译。 这篇文章用了个估算房价的例子，引出多元线性回归的算法，同时提到了以下几个知识点： 非线性数据 很多其他类型的机器学习算法可以处理（如神经网络或有核向量机） 过拟合 碰上这样一组权重值，它们对于你原始数据集中的房价都能完美预测， 但对于原始数据集之外的任何新房屋都预测不准。 这种情况的解决之道也有不少（如正则化以及使用交叉验证数据集） 同时，你只能对实际存在的关系建模并且可以使用机器学习算法库来解决实际问题]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>CS229</tag>
      </tags>
  </entry>
</search>
